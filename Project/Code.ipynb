{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "# from crfseg import CRF\n",
    "from torchcrf import CRF\n",
    "import torch.nn.functional as F\n",
    "import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "        # iterate through the files in the data directory\n",
    "        self.txtfiles = []\n",
    "        self.annfiles = []\n",
    "\n",
    "        for file in os.listdir(data_dir):\n",
    "            if file.endswith(\".txt\"):\n",
    "                self.txtfiles.append(file)\n",
    "        \n",
    "        self.tokeniser = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    def tokenise(self , text):\n",
    "        tokens = []  # List to store tokens\n",
    "        starting_offsets = []  # List to store starting offsets\n",
    "        current_token = ''  # Variable to store current token\n",
    "        offset = 0  # Starting offset\n",
    "\n",
    "        for char in text:\n",
    "            if char == ' ':\n",
    "                if current_token:  # If token is not empty\n",
    "                    tokens.append(current_token.lower())  # Append token in lowercase\n",
    "                    starting_offsets.append(offset - len(current_token))  # Store starting offset\n",
    "                    current_token = ''  # Reset current token\n",
    "                offset += 1  # Move offset to next character\n",
    "            else:\n",
    "                current_token += char  # Append character to current token\n",
    "                offset += 1  # Move offset to next character\n",
    "\n",
    "        # Handling the last token if it exists after the loop ends\n",
    "        if current_token:\n",
    "            tokens.append(current_token.lower())  # Append token in lowercase\n",
    "            starting_offsets.append(offset - len(current_token))  # Store starting offset\n",
    "\n",
    "        return starting_offsets , tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.txtfiles)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        txtfile = self.txtfiles[index]\n",
    "        sampleid = txtfile.split(\".\")[0]\n",
    "        \n",
    "        # read the text file\n",
    "        with open(os.path.join(self.data_dir, txtfile), 'r') as file:\n",
    "            txt = file.read()\n",
    "        \n",
    "        # read the annotation file\n",
    "        annfilename = sampleid + \".ann\"\n",
    "        with open(os.path.join(self.data_dir, annfilename), 'r') as file:\n",
    "            ann = file.read()\n",
    "        \n",
    "        offsets , tokenisedtxt = self.tokenise(txt)\n",
    "        tagslist = np.zeros(len(tokenisedtxt))\n",
    "        # now iterate through the ann file , in each line , divide into spaces and get the last word \n",
    "        # make tagslist[i] = 1 if the word is in the tokenisedtxt\n",
    "        for line in ann.split('\\n'):\n",
    "            if line == '':\n",
    "                continue\n",
    "            words = line.split()\n",
    "            if words[0][0] != 'T':\n",
    "                continue\n",
    "\n",
    "            ssofset = words[2]\n",
    "            endoffset = words[3]\n",
    "\n",
    "            # add a 1 to each index of tagslist for indexes where offset is between ssofset and endoffset (including both)\n",
    "            for i in range(len(offsets)):\n",
    "                if offsets[i] >= int(ssofset) and offsets[i] <= int(endoffset):\n",
    "                    tagslist[i] = 1\n",
    "        \n",
    "        tokenisedids = self.tokeniser.convert_tokens_to_ids(tokenisedtxt)\n",
    "        return torch.tensor(tokenisedids), torch.tensor(tagslist)\n",
    "    \n",
    "    def collate_fn(self , batch):\n",
    "        # batch is a list of tuples\n",
    "        # each tuple has 2 tensors , one for tokenisedids and one for tagslist\n",
    "        # we need to return a tensor of tokenisedids and a tensor of tagslist\n",
    "        tokenisedids = []\n",
    "        tagslist = []\n",
    "        for tup in batch:\n",
    "            tokenisedids.append(torch.tensor(tup[0]))\n",
    "            tagslist.append(torch.tensor(tup[1]))\n",
    "        \n",
    "        tokenisedids = torch.nn.utils.rnn.pad_sequence(tokenisedids , batch_first=True , padding_value=0) \n",
    "        tagslist = torch.nn.utils.rnn.pad_sequence(tagslist , batch_first=True , padding_value=0)\n",
    "\n",
    "        tokenisedids = tokenisedids.type(torch.LongTensor)\n",
    "        tagslist = tagslist.type(torch.LongTensor)\n",
    "        \n",
    "        return tokenisedids , tagslist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset('Data/train2')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True , collate_fn=train_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HELLO SARTHAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMCRF(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(BiLSTMCRF, self).__init__()\n",
    "        \n",
    "        # Load pre-trained SciBERT embeddings\n",
    "        self.bert = BertModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "        \n",
    "        # BiLSTM layers\n",
    "        self.bilstm = nn.LSTM(input_size=768, hidden_size=96, bidirectional=True, batch_first=True)\n",
    "        self.bilstm2 = nn.LSTM(input_size=192, hidden_size=48, bidirectional=True, batch_first=True)\n",
    "        self.bilstm3 = nn.LSTM(input_size=96, hidden_size=24, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # Linear layer for downsizing\n",
    "        self.linear = nn.Linear(48, num_labels)\n",
    "        \n",
    "        # CRF layer\n",
    "        self.crf = CRF(num_labels , batch_first=True)\n",
    "        \n",
    "    def forward(self, input_ids , gt_tags = None):\n",
    "        # Get SciBERT embeddings\n",
    "        bert_outputs = self.bert(input_ids)[0]\n",
    "        \n",
    "        # Apply BiLSTM layers\n",
    "        lstm_out, _ = self.bilstm(bert_outputs)\n",
    "        lstm_out, _ = self.bilstm2(lstm_out)\n",
    "        lstm_out, _ = self.bilstm3(lstm_out)\n",
    "        \n",
    "        linear_out = self.linear(lstm_out)\n",
    "        \n",
    "        # Apply CRF layer\n",
    "        if gt_tags is not None:\n",
    "            loss = -self.crf(linear_out , gt_tags)\n",
    "        else:\n",
    "            crf_out = self.crf.decode(linear_out)\n",
    "            return crf_out\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device  = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTMCRF(5).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/175 [00:00<?, ?batch/s]/home2/sarthak395/miniconda3/envs/myenv/lib/python3.7/site-packages/ipykernel_launcher.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home2/sarthak395/miniconda3/envs/myenv/lib/python3.7/site-packages/ipykernel_launcher.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "100%|██████████| 175/175 [00:38<00:00,  4.56batch/s, loss=181]\n",
      "100%|██████████| 175/175 [00:43<00:00,  4.04batch/s, loss=179]\n",
      "100%|██████████| 175/175 [00:43<00:00,  4.04batch/s, loss=178]\n",
      "100%|██████████| 175/175 [00:44<00:00,  3.97batch/s, loss=177]\n",
      "100%|██████████| 175/175 [00:43<00:00,  3.98batch/s, loss=176] \n",
      "100%|██████████| 175/175 [00:42<00:00,  4.14batch/s, loss=175]\n",
      "100%|██████████| 175/175 [00:43<00:00,  4.00batch/s, loss=174] \n",
      "100%|██████████| 175/175 [00:43<00:00,  4.05batch/s, loss=173] \n",
      "100%|██████████| 175/175 [00:43<00:00,  4.01batch/s, loss=172] \n",
      "100%|██████████| 175/175 [00:44<00:00,  3.96batch/s, loss=171] \n",
      "100%|██████████| 175/175 [00:42<00:00,  4.08batch/s, loss=170] \n",
      "100%|██████████| 175/175 [00:42<00:00,  4.14batch/s, loss=169] \n",
      "100%|██████████| 175/175 [00:43<00:00,  4.05batch/s, loss=168] \n",
      "100%|██████████| 175/175 [00:43<00:00,  4.01batch/s, loss=169] \n",
      "100%|██████████| 175/175 [00:43<00:00,  4.05batch/s, loss=168] \n",
      "100%|██████████| 175/175 [00:43<00:00,  3.99batch/s, loss=167] \n",
      "100%|██████████| 175/175 [00:44<00:00,  3.92batch/s, loss=167] \n",
      "100%|██████████| 175/175 [00:43<00:00,  4.00batch/s, loss=166] \n",
      "100%|██████████| 175/175 [00:44<00:00,  3.97batch/s, loss=165] \n",
      "100%|██████████| 175/175 [00:44<00:00,  3.95batch/s, loss=166] \n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    with tqdm.tqdm(train_dataloader, unit=\"batch\") as tepoch:\n",
    "        avg_loss = 0\n",
    "        for tokenisedids , tagslist in tepoch:\n",
    "            tokenisedids = tokenisedids.to(device)\n",
    "            tagslist = tagslist.to(device)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(tokenisedids , tagslist)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_dataloader)\n",
    "            tepoch.set_postfix(loss=avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), 'model2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
