{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sentence , we have 30x768 features .\n",
    "We will make 1 homonym classifier with 3 classes (forward relation , backward relation and no relation) and a synonym classifier with 2 classes (relation or no relation)\n",
    "\n",
    "Thus our dataset will be like (2 keyphrases , issynonym , ishyponym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "# from crfseg import CRF\n",
    "import torch.nn.functional as F\n",
    "import tqdm as tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir ,window_size = 2 , max_len_keyphrase = 30):\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "        # iterate through the files in the data directory\n",
    "        self.txtfiles = []\n",
    "        self.annfiles = []\n",
    "        self.max_len_keyphrase = max_len_keyphrase\n",
    "\n",
    "        for file in os.listdir(data_dir):\n",
    "            if file.endswith(\".txt\"):\n",
    "                self.txtfiles.append(file)\n",
    "        \n",
    "        self.tokeniser = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        # self.hyponym_label_dict = hyponym_label_dict\n",
    "\n",
    "        self.keyphrase_pairs = []\n",
    "        self.ishyponym = [] # for each keyphrase pair, 1 if it is a hyponym pair, 0 otherwise\n",
    "        self.issynonym = [] # for each keyphrase pair, 1 if it is a synonym pair, 0 otherwise\n",
    "\n",
    "        '''Keyphrases are stored in the ann files in the following format:'''\n",
    "        for txtfile in self.txtfiles:\n",
    "            sampleid = txtfile.split(\".\")[0]\n",
    "            annfilename = sampleid + \".ann\"\n",
    "            with open(os.path.join(self.data_dir, annfilename), 'r') as file:\n",
    "                ann = file.read()\n",
    "                \n",
    "            with open(os.path.join(self.data_dir, txtfile), 'r') as file:\n",
    "                txt = file.read()\n",
    "                \n",
    "            offsets , tokenisedtxt = self.tokenise(txt)\n",
    "            token_ids = self.tokeniser.convert_tokens_to_ids(tokenisedtxt)\n",
    "            keyphrases = [] # contains all the keyphrases in the txtfile\n",
    "            keyphrases_matching_dict = {} # contains the txtfile index(T1,T2 etc) with index of keyphrase in keyphrases\n",
    "            hyponym_pairs = [] # contains all the hyponym pairs in the txtfile\n",
    "            synonym_pairs = [] # contains all the synonym pairs in the txtfile\n",
    "\n",
    "            '''First get all keyphrases'''\n",
    "            for line in ann.split('\\n'):\n",
    "                if line == '':\n",
    "                    continue\n",
    "                words = line.split()\n",
    "                \n",
    "                txtindex = words[0]\n",
    "                ssofset = words[2]\n",
    "                endoffset = words[3]\n",
    "\n",
    "                if txtindex[0] == 'T':\n",
    "                    '''get the index of first token whose offset is greater than or equal to ssofset'''\n",
    "                    start_index = 0\n",
    "                    for i in range(len(offsets)):\n",
    "                        if offsets[i] >= int(ssofset):\n",
    "                            start_index = i\n",
    "                            break\n",
    "                    \n",
    "                    '''get the index of first token whose offset is greater than or equal to endoffset'''\n",
    "                    end_index = 0\n",
    "                    for i in range(len(offsets)):\n",
    "                        if offsets[i] >= int(endoffset):\n",
    "                            end_index = i\n",
    "                            break\n",
    "                    \n",
    "                    '''get the keyphrase tokens with context tokens'''\n",
    "                    mins = max(0 , start_index - window_size)\n",
    "                    maxs = min(len(token_ids) , end_index + window_size)\n",
    "                    tks = []\n",
    "                    for j in range(mins , maxs):\n",
    "                        tks.append(token_ids[j])\n",
    "                    \n",
    "                    # pad tks with bert_padding_token to make it of length max_len_keyphrase\n",
    "                    if len(tks) < max_len_keyphrase:\n",
    "                        tks += [self.tokeniser.pad_token_id] * (max_len_keyphrase - len(tks))\n",
    "\n",
    "                    keyphrases.append(tks)\n",
    "                    keyphrases_matching_dict[txtindex] = len(keyphrases) - 1\n",
    "\n",
    "                elif txtindex[0] == 'R':\n",
    "                    arg1 = words[2].split(':')[1]\n",
    "                    arg2 = words[3].split(':')[1]\n",
    "\n",
    "                    # there is a forward relation between arg1 and arg2\n",
    "                    if arg1[0] == 'T' and arg2[0] == 'T':\n",
    "                        keyphrase1_index = keyphrases_matching_dict[arg1]\n",
    "                        keyphrase2_index = keyphrases_matching_dict[arg2]\n",
    "                        hyponym_pairs.append((keyphrase1_index , keyphrase2_index))\n",
    "                \n",
    "                elif txtindex[0] == '*':\n",
    "                    '''synonym class'''\n",
    "                    arg1 = words[2]\n",
    "                    arg2 = words[3]\n",
    "                    \n",
    "                    if arg1[0] == 'T' and arg2[0] == 'T':\n",
    "                        keyphrase1_index = keyphrases_matching_dict[arg1]\n",
    "                        keyphrase2_index = keyphrases_matching_dict[arg2]\n",
    "                        synonym_pairs.append((keyphrase1_index , keyphrase2_index))\n",
    "            \n",
    "            '''Now make all keyphrase pairs and add them to self.keyphrase_pairs'''\n",
    "            for i in range(len(keyphrases)):\n",
    "                for j in range(i+1 , len(keyphrases)):\n",
    "                    self.keyphrase_pairs.append((keyphrases[i] , keyphrases[j]))\n",
    "                    if ((i , j) in hyponym_pairs) or ((j , i) in hyponym_pairs):\n",
    "                        self.ishyponym.append(1)\n",
    "                    else:\n",
    "                        self.ishyponym.append(0)\n",
    "                    \n",
    "                    if ((i , j) in synonym_pairs) or ((j , i) in synonym_pairs):\n",
    "                        self.issynonym.append(1)\n",
    "                    else:\n",
    "                        self.issynonym.append(0)\n",
    "\n",
    "\n",
    "    \n",
    "    def tokenise(self , text):\n",
    "        tokens = []  # List to store tokens\n",
    "        starting_offsets = []  # List to store starting offsets\n",
    "        current_token = ''  # Variable to store current token\n",
    "        offset = 0  # Starting offset\n",
    "\n",
    "        for char in text:\n",
    "            if char == ' ':\n",
    "                if current_token:  # If token is not empty\n",
    "                    tokens.append(current_token.lower())  # Append token in lowercase\n",
    "                    starting_offsets.append(offset - len(current_token))  # Store starting offset\n",
    "                    current_token = ''  # Reset current token\n",
    "                offset += 1  # Move offset to next character\n",
    "            else:\n",
    "                current_token += char  # Append character to current token\n",
    "                offset += 1  # Move offset to next character\n",
    "\n",
    "        # Handling the last token if it exists after the loop ends\n",
    "        if current_token:\n",
    "            tokens.append(current_token.lower())  # Append token in lowercase\n",
    "            starting_offsets.append(offset - len(current_token))  # Store starting offset\n",
    "\n",
    "        return starting_offsets , tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keyphrase_pairs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.keyphrase_pairs[index][0] ,self.keyphrase_pairs[index][1] , self.ishyponym[index] , self.issynonym[index]\n",
    "    \n",
    "    def collate_fn(self , batch):\n",
    "        keyphrases1s = []\n",
    "        keyphrases2s = []\n",
    "        label1s = []\n",
    "        label2s = []\n",
    "        for keyphrase1 , keyphrase2 , label1 , label2 in batch:\n",
    "            keyphrases1s.append(torch.tensor(keyphrase1))\n",
    "            keyphrases2s.append(torch.tensor(keyphrase2))\n",
    "            label1s.append(torch.tensor(label1))\n",
    "            label2s.append(torch.tensor(label2))\n",
    "        \n",
    "        keyphrases1s = torch.stack(keyphrases1s)\n",
    "        keyphrases2s = torch.stack(keyphrases2s)\n",
    "        label1s = torch.stack(label1s)\n",
    "        label2s = torch.stack(label2s)\n",
    "\n",
    "        return keyphrases1s ,keyphrases2s , label1s , label2s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72230\n"
     ]
    }
   ],
   "source": [
    "label_dict = {\n",
    "    'Process' : 0,\n",
    "    'Task' : 1,\n",
    "    'Material' : 2,\n",
    "}\n",
    "train_dataset = MyDataset('Data/train2')\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset , batch_size = 32 , shuffle = True , collate_fn = train_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self , emb_size = 3):\n",
    "        super(FeatureExtractor , self).__init__()\n",
    "        self.embedding = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.embed_dim = 768\n",
    "\n",
    "        for param in self.embedding.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.conv1 = nn.Sequential(nn.Conv1d(in_channels = self.embed_dim , out_channels = 256 , kernel_size = 1 , padding = 0) ,\n",
    "        nn.ReLU()) # (B , 768 , 30) -> (B , 256 , 30)\n",
    "\n",
    "        self.conv2 = nn.Sequential(nn.Conv1d(in_channels = self.embed_dim , out_channels = 256 , kernel_size = 3 , padding = 1) , \n",
    "        nn.ReLU())\n",
    "\n",
    "        self.conv3 = nn.Sequential(nn.Conv1d(in_channels = self.embed_dim , out_channels = 256 , kernel_size = 5 , padding = 2) ,\n",
    "        nn.ReLU())\n",
    "\n",
    "        self.conv4 = nn.Sequential(nn.Conv1d(in_channels = self.embed_dim , out_channels = 256 , kernel_size = 7 , padding = 3) ,\n",
    "        nn.ReLU())\n",
    "\n",
    "        self.max_over_time_pooling = nn.MaxPool1d(kernel_size=60) # (B , 256*4 , 30) -> (B , 256*4)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256*4 , 512) , nn.ReLU() , nn.Linear(512 , 256) , nn.ReLU() , nn.Linear(256 , emb_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self , x1 , x2):\n",
    "        x = torch.cat([x1 , x2] , dim = 1) # shape : B , 60\n",
    "        embeddings  = self.embedding(x)[0] # shape : B , 60 , 768\n",
    "        embeddings = embeddings.permute(0 , 2 , 1)\n",
    "\n",
    "        conv1_out = self.conv1(embeddings) # shape : B , 256 , 60\n",
    "        conv2_out = self.conv2(embeddings) # shape : B , 256 , 60\n",
    "        conv3_out = self.conv3(embeddings) # shape : B , 256 , 60\n",
    "        conv4_out = self.conv4(embeddings) # shape : B , 256 , 60\n",
    "\n",
    "        concat_conv_outs = torch.cat([conv1_out , conv2_out , conv3_out , conv4_out] , dim = 1) # shape : B , 256*4 , 60\n",
    "        max_pooled = self.max_over_time_pooling(concat_conv_outs)\n",
    "        max_pooled = max_pooled.squeeze(2) # B , 256*4\n",
    "        features = self.fc(max_pooled)\n",
    "\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 30]) torch.Size([32, 30])\n",
      "torch.Size([32, 3])\n"
     ]
    }
   ],
   "source": [
    "feature_extractor_model = FeatureExtractor()\n",
    "sample_inp = torch.randint(0 , 30522 , (32 , 30))\n",
    "sample_inp2 = torch.randint(0 , 30522 , (32 , 30))\n",
    "print(sample_inp.shape , sample_inp2.shape)\n",
    "output_emb = feature_extractor_model(sample_inp , sample_inp2)\n",
    "print(output_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = FeatureExtractor().to(device)\n",
    "optimiser = torch.optim.Adam(model.parameters() , lr = 0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = MyDataset('Data/dev')\n",
    "val_dataloader = DataLoader(val_dataset , batch_size = 32 , shuffle = False , collate_fn = val_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2258/2258 [01:45<00:00, 21.37batch/s, loss=0.462]\n",
      "100%|██████████| 2258/2258 [01:54<00:00, 19.71batch/s, loss=0.458]\n",
      "100%|██████████| 2258/2258 [01:55<00:00, 19.57batch/s, loss=0.452]\n",
      "100%|██████████| 2258/2258 [01:56<00:00, 19.45batch/s, loss=0.439]\n",
      "100%|██████████| 2258/2258 [01:57<00:00, 19.20batch/s, loss=0.403] \n",
      "100%|██████████| 2258/2258 [01:58<00:00, 19.09batch/s, loss=0.388] \n",
      "100%|██████████| 2258/2258 [01:58<00:00, 19.13batch/s, loss=0.375]\n",
      "100%|██████████| 2258/2258 [01:56<00:00, 19.31batch/s, loss=0.381]\n",
      "100%|██████████| 2258/2258 [01:56<00:00, 19.37batch/s, loss=0.36]  \n",
      "100%|██████████| 2258/2258 [01:56<00:00, 19.42batch/s, loss=0.327]\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "num_epochs = 10\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    avg_loss = 0\n",
    "    with tqdm.tqdm(train_dataloader, unit=\"batch\") as tepoch:\n",
    "        for keyphrases1 , keyphrases2 , label1 , label2 in tepoch:\n",
    "            keyphrases1 = keyphrases1.to(device)\n",
    "            keyphrases2 = keyphrases2.to(device)\n",
    "            label1 = label1.to(device = device , dtype = torch.float32) # B , 1\n",
    "            label2 = label2.to(device = device , dtype = torch.float32) # B , 1\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            out = model(keyphrases1 , keyphrases2) # (B , 3)\n",
    "\n",
    "            hyponym_loss = criterion(out[: , 0] , label1)\n",
    "            synonym_loss = criterion(out[: , 1] , label2)\n",
    "            loss = hyponym_loss + synonym_loss\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            avg_loss += loss.item() / len(train_dataloader)\n",
    "            tepoch.set_postfix(loss=avg_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
