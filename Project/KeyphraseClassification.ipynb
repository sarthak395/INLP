{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "# from crfseg import CRF\n",
    "from torchcrf import CRF\n",
    "import torch.nn.functional as F\n",
    "import tqdm as tqdm\n",
    "from torchtext.vocab import GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir , label_dict , window_size = 2 , max_len_keyphrase = 30):\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "        # iterate through the files in the data directory\n",
    "        self.txtfiles = []\n",
    "        self.annfiles = []\n",
    "        self.max_len_keyphrase = max_len_keyphrase\n",
    "\n",
    "        for file in os.listdir(data_dir):\n",
    "            if file.endswith(\".txt\"):\n",
    "                self.txtfiles.append(file)\n",
    "        \n",
    "        self.tokeniser = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.keyphrases_with_context = [] # it contains all the keyphrase tokens with context tokens\n",
    "        self.labels = []\n",
    "        self.label_dict = label_dict\n",
    "\n",
    "        '''Keyphrases are stored in the ann files in the following format:'''\n",
    "        for txtfile in self.txtfiles:\n",
    "            sampleid = txtfile.split(\".\")[0]\n",
    "            annfilename = sampleid + \".ann\"\n",
    "            with open(os.path.join(self.data_dir, annfilename), 'r') as file:\n",
    "                ann = file.read()\n",
    "                \n",
    "            with open(os.path.join(self.data_dir, txtfile), 'r') as file:\n",
    "                txt = file.read()\n",
    "                \n",
    "            offsets , tokenisedtxt = self.tokenise(txt)\n",
    "            token_ids = self.tokeniser.convert_tokens_to_ids(tokenisedtxt)\n",
    "\n",
    "\n",
    "            for line in ann.split('\\n'):\n",
    "                if line == '':\n",
    "                    continue\n",
    "                words = line.split()\n",
    "                if words[0][0] != 'T':\n",
    "                    continue\n",
    "                \n",
    "                label = self.label_dict[words[1]]\n",
    "                ssofset = words[2]\n",
    "                endoffset = words[3]\n",
    "\n",
    "                '''get the index of first token whose offset is greater than or equal to ssofset'''\n",
    "                start_index = 0\n",
    "                for i in range(len(offsets)):\n",
    "                    if offsets[i] >= int(ssofset):\n",
    "                        start_index = i\n",
    "                        break\n",
    "                \n",
    "                '''get the index of first token whose offset is greater than or equal to endoffset'''\n",
    "                end_index = 0\n",
    "                for i in range(len(offsets)):\n",
    "                    if offsets[i] >= int(endoffset):\n",
    "                        end_index = i\n",
    "                        break\n",
    "                \n",
    "                '''get the keyphrase tokens with context tokens'''\n",
    "                mins = max(0 , start_index - window_size)\n",
    "                maxs = min(len(token_ids) , end_index + window_size)\n",
    "                tks = []\n",
    "                for j in range(mins , maxs):\n",
    "                    tks.append(token_ids[j])\n",
    "                \n",
    "                # pad tks with bert_padding_token to make it of length max_len_keyphrase\n",
    "                if len(tks) < max_len_keyphrase:\n",
    "                    tks += [self.tokeniser.pad_token_id] * (max_len_keyphrase - len(tks))\n",
    "\n",
    "                self.keyphrases_with_context.append(tks)\n",
    "                self.labels.append(label)\n",
    "\n",
    "    \n",
    "    def tokenise(self , text):\n",
    "        tokens = []  # List to store tokens\n",
    "        starting_offsets = []  # List to store starting offsets\n",
    "        current_token = ''  # Variable to store current token\n",
    "        offset = 0  # Starting offset\n",
    "\n",
    "        for char in text:\n",
    "            if char == ' ':\n",
    "                if current_token:  # If token is not empty\n",
    "                    tokens.append(current_token.lower())  # Append token in lowercase\n",
    "                    starting_offsets.append(offset - len(current_token))  # Store starting offset\n",
    "                    current_token = ''  # Reset current token\n",
    "                offset += 1  # Move offset to next character\n",
    "            else:\n",
    "                current_token += char  # Append character to current token\n",
    "                offset += 1  # Move offset to next character\n",
    "\n",
    "        # Handling the last token if it exists after the loop ends\n",
    "        if current_token:\n",
    "            tokens.append(current_token.lower())  # Append token in lowercase\n",
    "            starting_offsets.append(offset - len(current_token))  # Store starting offset\n",
    "\n",
    "        return starting_offsets , tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keyphrases_with_context)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.keyphrases_with_context[index] , self.labels[index]\n",
    "    \n",
    "    def collate_fn(self , batch):\n",
    "        keyphrases = []\n",
    "        labels = []\n",
    "        for keyphrase , label in batch:\n",
    "            keyphrases.append(torch.tensor(keyphrase))\n",
    "            labels.append(torch.tensor(label))\n",
    "        \n",
    "        keyphrases = torch.stack(keyphrases)\n",
    "        labels = torch.stack(labels)\n",
    "\n",
    "        return keyphrases , labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6732\n"
     ]
    }
   ],
   "source": [
    "label_dict = {\n",
    "    'Process' : 0,\n",
    "    'Task' : 1,\n",
    "    'Material' : 2,\n",
    "}\n",
    "train_dataset = MyDataset('Data/train2' , label_dict)\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset , batch_size = batch_size , shuffle = True , collate_fn = train_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyphraseClassificationLinear(nn.Module):\n",
    "    def __init__(self , num_labels , hidden_size = 768):\n",
    "        super(KeyphraseClassificationLinear , self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_labels = num_labels\n",
    "        self.linear = nn.Linear(hidden_size , num_labels)\n",
    "    \n",
    "    def forward(self , input_ids):\n",
    "        outputs = self.bert(input_ids)[0] # shape : B , 30 , 768\n",
    "        # convert outputs to shape : B , 768 i.e make sentence level representation\n",
    "        outputs = torch.mean(outputs , dim = 1)\n",
    "        logits = self.linear(outputs)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyphraseClassificationAttentionLSTM(nn.Module):\n",
    "    def __init__(self , num_labels , embedding_type = 'bert'):\n",
    "        '''Model takes in B kayphrase tokens (all of length 30) and outputs probabilities for num_labels classes'''\n",
    "        '''Input Shape : (B , 30) Output Shape : (B , num_labels)'''\n",
    "        super(KeyphraseClassificationAttentionLSTM , self).__init__()\n",
    "\n",
    "        '''First generate Embeddings for the input tokens'''\n",
    "        self.embedding = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.embed_dim = 768\n",
    "\n",
    "        for param in self.embedding.parameters():\n",
    "            param.requires_grad = False\n",
    "                \n",
    "\n",
    "        '''Then Generate Attentional Vectors for the input tokens'''\n",
    "        self.conv1 = nn.Conv1d(in_channels = self.embed_dim , out_channels = 256 , kernel_size = 1 , padding = 0) # (B , 768 , 30) -> (B , 256 , 30)\n",
    "        self.conv2 = nn.Conv1d(in_channels = self.embed_dim , out_channels = 256 , kernel_size = 3 , padding=1) # (B , 768 , 30) -> (B , 256 , 30)\n",
    "        self.conv3 = nn.Conv1d(in_channels = self.embed_dim , out_channels = 256 , kernel_size = 5 , padding=2) # (B , 768 , 30) -> (B , 256 , 30)\n",
    "        self.conv4 = nn.Conv1d(in_channels = self.embed_dim , out_channels = 256 , kernel_size = 7 , padding=3) # (B , 768 , 30) -> (B , 256 , 30)\n",
    "        self.max_over_time_pooling = nn.MaxPool1d(kernel_size=256*4) # (B , 30 , 256*4) -> (B , 30)\n",
    "\n",
    "        '''Then we use Bi-lstm to generate hidden states for the input tokens'''\n",
    "        self.bilstm = nn.LSTM(input_size = self.embed_dim  , hidden_size = 256 , num_layers = 2 , bidirectional = True , batch_first = True) # (B , 30 , 768) -> (B , 30 , 512)\n",
    "\n",
    "        '''Output Layer'''\n",
    "        self.linear = nn.Linear(512 , num_labels)\n",
    "    \n",
    "\n",
    "    def forward(self , input_ids , bert_tokeniser = None):\n",
    "\n",
    "        embeddings = self.embedding(input_ids)[0] # shape : B , 30 , 768\n",
    "        \n",
    "\n",
    "        # All conv outputs have shape : B , 256 , 30\n",
    "        conv1_out = self.conv1(embeddings.permute(0,2,1)) \n",
    "        conv2_out = self.conv2(embeddings.permute(0,2,1))\n",
    "        conv3_out = self.conv3(embeddings.permute(0,2,1))\n",
    "        conv4_out = self.conv4(embeddings.permute(0,2,1))\n",
    "\n",
    "        concat_conv_outs = torch.cat([conv1_out , conv2_out , conv3_out , conv4_out] , dim = 1) # shape : B , 256*4 , 30\n",
    "        concat_conv_outs = concat_conv_outs.permute(0,2,1) # shape : B , 30 , 256*4\n",
    "        max_pooled = self.max_over_time_pooling(concat_conv_outs) # shape : B , 30\n",
    "        max_pooled = F.softmax(max_pooled , dim = 1) # shape : B , 30\n",
    "\n",
    "        bilstm_outs , _ = self.bilstm(embeddings) # shape : B , 30 , 512\n",
    "        \n",
    "        '''Multiply the max_pooled and bilstm_outs to weigh the importance of each token'''\n",
    "        attentional_vector = torch.mul(max_pooled , bilstm_outs) # shape : B , 30 , 512\n",
    "\n",
    "        '''Now to get the final output we sum over the 30 tokens'''\n",
    "        final_output = torch.sum(attentional_vector , dim = 1) # shape : B , 512\n",
    "        final_output = self.linear(final_output) # shape : B , num_labels\n",
    "\n",
    "        return final_output\n",
    "\n",
    "    def convert_input_ids_to_glove_indices(input_ids, bert_tokenizer, glove_word_to_index):\n",
    "        glove_indices = []\n",
    "        for batch_input_ids in input_ids:\n",
    "            glove_indices_batch = []\n",
    "            for token_id in batch_input_ids:\n",
    "                # Convert BERT token ID to token\n",
    "                token = bert_tokenizer.convert_ids_to_tokens(token_id.item())\n",
    "                # Find the closest matching token in the GloVe vocabulary\n",
    "                glove_index = glove_word_to_index.get(token.lower(), None)\n",
    "                if glove_index is not None:\n",
    "                    glove_indices_batch.append(glove_index)\n",
    "                else:\n",
    "                    # Handle out-of-vocabulary tokens (e.g., assign a special index)\n",
    "                    glove_indices_batch.append(UNKNOWN_INDEX)\n",
    "            glove_indices.append(glove_indices_batch)\n",
    "        return torch.tensor(glove_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3])\n"
     ]
    }
   ],
   "source": [
    "sample_inp = torch.randint(0 , 30522 , (32 , 30))\n",
    "# model = KeyphraseClassificationLinear(3)\n",
    "model = KeyphraseClassificationAttentionLSTM(3)\n",
    "logits = model(sample_inp)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# model = KeyphraseClassificationLinear(3).to(device)\n",
    "model = KeyphraseClassificationAttentionLSTM(3).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters() , lr = 0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model , data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for keyphrases , labels in data_loader:\n",
    "            keyphrases = keyphrases.to(device)\n",
    "            labels = labels.to(device)\n",
    "            logits = model(keyphrases)\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = MyDataset('Data/dev' , label_dict)\n",
    "val_dataloader = DataLoader(val_dataset , batch_size = batch_size , shuffle = False , collate_fn = val_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 211/211 [03:05<00:00,  1.14batch/s, loss=0.974, val_acc=0.609]\n",
      "100%|██████████| 211/211 [02:59<00:00,  1.17batch/s, loss=0.915, val_acc=0.685]\n",
      "100%|██████████| 211/211 [03:00<00:00,  1.17batch/s, loss=0.878, val_acc=0.678]\n",
      "100%|██████████| 211/211 [03:00<00:00,  1.17batch/s, loss=0.857, val_acc=0.698]\n",
      "100%|██████████| 211/211 [03:00<00:00,  1.17batch/s, loss=0.844, val_acc=0.704]\n",
      "100%|██████████| 211/211 [03:00<00:00,  1.17batch/s, loss=0.83, val_acc=0.735] \n",
      "100%|██████████| 211/211 [03:00<00:00,  1.17batch/s, loss=0.822, val_acc=0.727]\n",
      "100%|██████████| 211/211 [03:00<00:00,  1.17batch/s, loss=0.812, val_acc=0.72] \n",
      "100%|██████████| 211/211 [03:00<00:00,  1.17batch/s, loss=0.795, val_acc=0.717]\n",
      "100%|██████████| 211/211 [03:00<00:00,  1.17batch/s, loss=0.79, val_acc=0.72]  \n",
      "100%|██████████| 211/211 [03:00<00:00,  1.17batch/s, loss=0.776, val_acc=0.721]\n",
      "100%|██████████| 211/211 [03:00<00:00,  1.17batch/s, loss=0.77, val_acc=0.711] \n",
      "100%|██████████| 211/211 [03:00<00:00,  1.17batch/s, loss=0.765, val_acc=0.717]\n",
      "100%|██████████| 211/211 [03:00<00:00,  1.17batch/s, loss=0.744, val_acc=0.737]\n",
      "100%|██████████| 211/211 [03:00<00:00,  1.17batch/s, loss=0.735, val_acc=0.709]\n",
      "100%|██████████| 211/211 [03:00<00:00,  1.17batch/s, loss=0.742, val_acc=0.701]\n",
      "100%|██████████| 211/211 [03:00<00:00,  1.17batch/s, loss=0.721, val_acc=0.705]\n",
      "100%|██████████| 211/211 [03:00<00:00,  1.17batch/s, loss=0.709, val_acc=0.73] \n",
      "100%|██████████| 211/211 [03:00<00:00,  1.17batch/s, loss=0.714, val_acc=0.706]\n",
      "100%|██████████| 211/211 [03:00<00:00,  1.17batch/s, loss=0.689, val_acc=0.714]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    with tqdm.tqdm(train_dataloader, unit=\"batch\") as tepoch:\n",
    "        avg_loss = 0\n",
    "        for tokenisedids , tagslist in tepoch:\n",
    "            tokenisedids = tokenisedids.to(device)\n",
    "            tagslist = tagslist.to(device)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(tokenisedids)\n",
    "            # print(out.size() , tagslist.size())\n",
    "            loss = criterion(out, tagslist)\n",
    "        \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_dataloader)\n",
    "            acc = get_accuracy(model , val_dataloader)\n",
    "            tepoch.set_postfix(loss=avg_loss , val_acc = acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Save the model'''\n",
    "torch.save(model.state_dict() , 'model_att_bert.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Just linear : 66.7 %\n",
    "\n",
    "With attentional LSTM and BERT : 73 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
