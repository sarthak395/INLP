{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "# from crfseg import CRF\n",
    "import torch.nn.functional as F\n",
    "import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "# Function to extract noun phrases from POS tags\n",
    "# def extract_noun_phrases(pos_tags):\n",
    "#     # Define patterns for noun phrases\n",
    "#     patterns = [\n",
    "#         [['NN'], ['NNS'], ['NNP'], ['NNPS']],\n",
    "#         [['DT', 'NN'], ['DT', 'NNS'], ['DT', 'NNP'], ['DT', 'NNPS'], ['JJ', 'NN'], ['JJ', 'NNS'], ['JJ', 'NNP'], ['JJ', 'NNPS']],\n",
    "#         [['DT', 'JJ', 'NN'], ['DT', 'JJ', 'NNS'], ['DT', 'JJ', 'NNP'], ['DT', 'JJ', 'NNPS']],\n",
    "#     ]\n",
    "\n",
    "#     noun_phrases = []\n",
    "\n",
    "#     # Iterate through the POS tags\n",
    "#     for i in range(len(pos_tags)):\n",
    "#         # Initialize flag to check if a longer keyword is found\n",
    "#         longer_keyword_found = False\n",
    "\n",
    "#         # Iterate through all possible lengths (up to trigrams)\n",
    "#         for length in range(3, 0, -1):  # Start from length 3 and move to length 1\n",
    "#             if i + length <= len(pos_tags):\n",
    "#                 current_sequence = [tag for word, tag in pos_tags[i:i+length]]\n",
    "#                 current_keyword = ' '.join(word for word, tag in pos_tags[i:i+length])\n",
    "\n",
    "#                 # Check if current sequence matches any pattern\n",
    "#                 for pattern in patterns[length - 1]:  # Adjust index for patterns\n",
    "#                     if current_sequence == pattern:\n",
    "#                         noun_phrases.append(current_keyword)\n",
    "#                         # longer_keyword_found = True\n",
    "#                         # break\n",
    "\n",
    "#                 # if longer_keyword_found:\n",
    "#                 #     break  # Break loop if a longer keyword is found\n",
    "    \n",
    "#     # Sort noun phrases based on their length in reverse order\n",
    "#     noun_phrases.sort(key=len, reverse=True)\n",
    "#     return noun_phrases\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_noun_phrases(pos_tags):\n",
    "    # Define patterns for noun phrases\n",
    "    patterns = [\n",
    "        r'NN.? VB.? NN.?',\n",
    "        r'NN.? IN NN.?',\n",
    "        r'DT JJ NN.?',\n",
    "        r'JJ NN.?',\n",
    "        r'DT NN.?',\n",
    "        r'NN.?'\n",
    "    ]\n",
    "\n",
    "    noun_phrases = []\n",
    "\n",
    "    # Convert the list of tuples to a space-separated string\n",
    "    pos_string = ' '.join(tag for word, tag in pos_tags)\n",
    "\n",
    "    # Check if the POS string matches any pattern\n",
    "    for pattern in patterns:\n",
    "        for match in re.finditer(pattern, pos_string):\n",
    "            # Get the start and end indices of the match\n",
    "            start, end = match.span()\n",
    "\n",
    "            # Convert the indices to word indices\n",
    "            start = pos_string[:start].count(' ')\n",
    "            end = pos_string[:end].count(' ')\n",
    "\n",
    "            # Extract the corresponding words\n",
    "            noun_phrase = ' '.join(word for word, tag in pos_tags[start:end])\n",
    "            \n",
    "            # Check if the noun phrase is not empty\n",
    "            if noun_phrase.strip():\n",
    "                noun_phrases.append(noun_phrase)\n",
    "\n",
    "    # Remove empty strings from the list\n",
    "    noun_phrases = [phrase for phrase in noun_phrases if phrase]\n",
    "\n",
    "    return noun_phrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "        # iterate through the files in the data directory\n",
    "        self.txtfiles = []\n",
    "        self.annfiles = []\n",
    "\n",
    "        for file in os.listdir(data_dir):\n",
    "            if file.endswith(\".txt\"):\n",
    "                self.txtfiles.append(file)\n",
    "        \n",
    "        self.tokeniser = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    def tokenise(self , text):\n",
    "        tokens = []  # List to store tokens\n",
    "        starting_offsets = []  # List to store starting offsets\n",
    "        current_token = ''  # Variable to store current token\n",
    "        offset = 0  # Starting offset\n",
    "\n",
    "        for char in text:\n",
    "            if char == ' ':\n",
    "                if current_token:  # If token is not empty\n",
    "                    tokens.append(current_token.lower())  # Append token in lowercase\n",
    "                    starting_offsets.append(offset - len(current_token))  # Store starting offset\n",
    "                    current_token = ''  # Reset current token\n",
    "                offset += 1  # Move offset to next character\n",
    "            else:\n",
    "                current_token += char  # Append character to current token\n",
    "                offset += 1  # Move offset to next character\n",
    "\n",
    "        # Handling the last token if it exists after the loop ends\n",
    "        if current_token:\n",
    "            tokens.append(current_token.lower())  # Append token in lowercase\n",
    "            starting_offsets.append(offset - len(current_token))  # Store starting offset\n",
    "\n",
    "        return starting_offsets , tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.txtfiles)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        txtfile = self.txtfiles[index]\n",
    "        sampleid = txtfile.split(\".\")[0]\n",
    "        \n",
    "        # read the text file\n",
    "        with open(os.path.join(self.data_dir, txtfile), 'r') as file:\n",
    "            txt = file.read()\n",
    "        \n",
    "        # read the annotation file\n",
    "        annfilename = sampleid + \".ann\"\n",
    "        with open(os.path.join(self.data_dir, annfilename), 'r') as file:\n",
    "            ann = file.read()\n",
    "        \n",
    "        offsets , tokenisedtxt = self.tokenise(txt)\n",
    "        tagslist = np.zeros(len(tokenisedtxt))\n",
    "        # now iterate through the ann file , in each line , divide into spaces and get the last word \n",
    "        # make tagslist[i] = 1 if the word is in the tokenisedtxt\n",
    "        for line in ann.split('\\n'):\n",
    "            if line == '':\n",
    "                continue\n",
    "            words = line.split()\n",
    "            if words[0][0] != 'T':\n",
    "                continue\n",
    "\n",
    "            ssofset = words[2]\n",
    "            endoffset = words[3]\n",
    "\n",
    "            # add a 1 to each index of tagslist for indexes where offset is between ssofset and endoffset (including both)\n",
    "            for i in range(len(offsets)):\n",
    "                if offsets[i] >= int(ssofset) and offsets[i] <= int(endoffset):\n",
    "                    tagslist[i] = 1\n",
    "        \n",
    "        # Convert tokens to IDs using BERT tokenizer\n",
    "        tokenisedids = self.tokeniser.convert_tokens_to_ids(tokenisedtxt)\n",
    "        \n",
    "        # Perform POS tagging\n",
    "        pos_tags = nltk.pos_tag(tokenisedtxt)\n",
    "        \n",
    "        # Extract noun phrases from POS tags\n",
    "        noun_phrases = extract_noun_phrases(pos_tags)\n",
    "        \n",
    "        return torch.tensor(tokenisedids), torch.tensor(tagslist), noun_phrases\n",
    "    \n",
    "    def collate_fn(self , batch):\n",
    "        # batch is a list of tuples\n",
    "        # each tuple has 3 tensors , one for tokenisedids, one for tagslist, and one for noun_phrases\n",
    "        # we need to return a tensor of tokenisedids, a tensor of tagslist, and a list of lists for noun_phrases\n",
    "        tokenisedids = []\n",
    "        tagslist = []\n",
    "        noun_phrases = []\n",
    "        for tup in batch:\n",
    "            tokenisedids.append(torch.tensor(tup[0]))\n",
    "            tagslist.append(torch.tensor(tup[1]))\n",
    "            noun_phrases.append(tup[2])\n",
    "        \n",
    "        tokenisedids = torch.nn.utils.rnn.pad_sequence(tokenisedids , batch_first=True , padding_value=0) \n",
    "        tagslist = torch.nn.utils.rnn.pad_sequence(tagslist , batch_first=True , padding_value=0)\n",
    "\n",
    "        tokenisedids = tokenisedids.type(torch.LongTensor)\n",
    "        tagslist = tagslist.type(torch.LongTensor)\n",
    "        \n",
    "        return tokenisedids , tagslist, noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset('/Users/ashnadua/Desktop/INLP-project/scienceie2017_train/train2')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True , collate_fn=train_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gold_keywords(ann_file):\n",
    "    gold_keywords = set()\n",
    "    with open(ann_file, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('T'):\n",
    "                parts = line.split()\n",
    "                keyword = ' '.join(parts[4:])\n",
    "                gold_keywords.add(keyword.lower())\n",
    "    return gold_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(extracted_keywords, gold_keywords):\n",
    "    correctly_extracted = len(extracted_keywords.intersection(gold_keywords))\n",
    "    total_gold_keywords = len(gold_keywords)\n",
    "    accuracy = correctly_extracted / total_gold_keywords if total_gold_keywords > 0 else 0\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy over the entire dataset: 0.1905072174591904\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir = '/Users/ashnadua/Desktop/INLP-project/scienceie2017_dev/dev'\n",
    "total_accuracy = 0\n",
    "total_files = 0\n",
    "\n",
    "for file in os.listdir(data_dir):\n",
    "    if file.endswith(\".txt\"):\n",
    "        txt_path = os.path.join(data_dir, file)\n",
    "        ann_path = os.path.join(data_dir, file[:-3] + \"ann\")\n",
    "\n",
    "        with open(txt_path, 'r') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        tokens = word_tokenize(text)\n",
    "        pos_tags = nltk.pos_tag(tokens)\n",
    "        noun_phrases = extract_noun_phrases(pos_tags)\n",
    "        noun_phrases = set(noun_phrases)  # Convert to set\n",
    "\n",
    "        gold_keywords = load_gold_keywords(ann_path)\n",
    "\n",
    "        accuracy = calculate_accuracy(noun_phrases, gold_keywords)\n",
    "        total_accuracy += accuracy\n",
    "        total_files += 1\n",
    "\n",
    "# Calculate the average accuracy\n",
    "average_accuracy = total_accuracy / total_files\n",
    "print(f\"Average accuracy over the entire dataset: {average_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['production of', 'development of', 'depth of oxygen', 'number of', 'the major barrier', 'the typical temperature', 'a limited number', 'a certain oxidation', 'the expected', 'poor oxidation', 'major barrier', 'ti-based', 'structural', 'typical temperature', 'careful study', 'ti-based', 'ti-based alloys', 'scale thickness', 'limited number', 'certain oxidation', 'compositional range', 'numerous', 'expected', 'the demand', 'the service', 'these', 'the role', 'that composition', 'the oxidation', 'the attempt', 'this limitation', 'the production', 'the oxidation', 'the literature', 'oxidation', 'behavior', 'barrier', 'use', 'demand', 'service', 'temperature', 'temperature', 'limit', 'study', 'role', 'composition', 'oxidation', 'behavior', 'attempt', 'limitation', 'alloys', 'production', 'oxidation', 'resistance', 'development', 'pre-oxidation', 'oxidation', 'behavior', 'oxidation', 'rate', 'law', 'depth', 'oxygen', 'ingress', 'thickness', 'number', 'oxidation', 'condition', 'range', 'literature', ']']\n"
     ]
    }
   ],
   "source": [
    "text = \"Poor oxidation behavior is the major barrier to the increased use of Ti-based alloys in high-temperature structural applications. The demand to increase the service temperature of these alloys beyond 550°C (the typical temperature limit) requires careful study to understand the role that composition has on the oxidation behavior of Ti-based alloys [1–3]. The attempt to overcome this limitation in Ti-based alloys has led to the production of alloys with substantially improved oxidation resistance such as β-21S and also development of coatings and pre-oxidation techniques [1,4–6]. While it is tempting to extrapolate the oxidation behavior (e.g. oxidation rate law, depth of oxygen ingress and scale thickness) observed for a limited number of compositions under a certain oxidation condition to a broader compositional range, there are numerous examples in the literature where deviations from the expected relations are observed [7,8].\"\n",
    "text = text.lower()\n",
    "tokens = nltk.word_tokenize(text)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "noun_phrases = extract_noun_phrases(pos_tags)\n",
    "print(noun_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_phrases = set(noun_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4\n"
     ]
    }
   ],
   "source": [
    "ann_file = '/Users/ashnadua/Desktop/INLP-project/scienceie2017_train/train2/S0010938X1500195X.ann'  # Path to your annotation file\n",
    "gold_keywords = load_gold_keywords(ann_file)\n",
    "\n",
    "accuracy = calculate_accuracy(noun_phrases, gold_keywords)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = list(noun_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poor oxidation\n",
      "the expected\n",
      "certain oxidation\n",
      "oxygen\n",
      "scale thickness\n",
      "typical temperature\n",
      "production\n",
      "the attempt\n",
      "role\n",
      "depth\n",
      "thickness\n",
      "expected\n",
      "the service\n",
      "barrier\n",
      "limited number\n",
      "this limitation\n",
      "number of\n",
      "careful study\n",
      "the demand\n",
      "alloys\n",
      "resistance\n",
      "ingress\n",
      "the production\n",
      "a certain oxidation\n",
      "production of\n",
      "depth of oxygen\n",
      "structural\n",
      "numerous\n",
      "range\n",
      "limitation\n",
      "pre-oxidation\n",
      "the role\n",
      "behavior\n",
      "limit\n",
      "composition\n",
      "compositional range\n",
      "a limited number\n",
      "use\n",
      "oxidation\n",
      "attempt\n",
      "literature\n",
      "development\n",
      "law\n",
      "development of\n",
      "]\n",
      "these\n",
      "temperature\n",
      "condition\n",
      "number\n",
      "the literature\n",
      "ti-based\n",
      "the major barrier\n",
      "service\n",
      "ti-based alloys\n",
      "major barrier\n",
      "the oxidation\n",
      "rate\n",
      "demand\n",
      "the typical temperature\n",
      "study\n",
      "that composition\n"
     ]
    }
   ],
   "source": [
    "for key in noun_phrases:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poor oxidation      thickness           resistance          pre-oxidation       literature          ti-based            that composition    \n",
      "the expected        expected            ingress             the role            development         the major barrier   \n",
      "certain oxidation   the service         the production      behavior            law                 service             \n",
      "oxygen              barrier             a certain oxidation limit               development of      ti-based alloys     \n",
      "scale thickness     limited number      production of       composition         ]                   major barrier       \n",
      "typical temperature this limitation     depth of oxygen     compositional range these               the oxidation       \n",
      "production          number of           structural          a limited number    temperature         rate                \n",
      "the attempt         careful study       numerous            use                 condition           demand              \n",
      "role                the demand          range               oxidation           number              the typical temperature\n",
      "depth               alloys              limitation          attempt             the literature      study               \n"
     ]
    }
   ],
   "source": [
    "keys_per_column = 10\n",
    "\n",
    "# Calculate the number of columns needed\n",
    "num_columns = (len(predicted) + keys_per_column - 1) // keys_per_column\n",
    "\n",
    "# Print keys in side-by-side columns\n",
    "for i in range(keys_per_column):\n",
    "    for j in range(num_columns):\n",
    "        idx = j * keys_per_column + i\n",
    "        if idx < len(predicted):\n",
    "            print(f\"{predicted[idx]:<20}\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(20):\n",
    "#     tokenisedids, tagslist, noun_phrases = train_dataset[i]\n",
    "#     print(f\"Sample {i+1}:\")\n",
    "#     print(\"Tokenized Text:\", tokenisedids)\n",
    "#     print(\"Tags List:\", tagslist)\n",
    "#     print(\"Noun Phrases:\", noun_phrases)\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "\n",
    "# def extract_noun_phrases_spacy(text):\n",
    "#     nlp = spacy.load('en_core_web_sm')\n",
    "#     doc = nlp(text)\n",
    "#     noun_phrases = [chunk.text for chunk in doc.noun_chunks]\n",
    "#     return noun_phrases\n",
    "\n",
    "# # Example usage:\n",
    "# text = \"This is an example sentence with keywords like beautiful flowers and green grass.\"\n",
    "# noun_phrases_spacy = extract_noun_phrases_spacy(text)\n",
    "# print(noun_phrases_spacy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
