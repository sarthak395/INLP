{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "# from crfseg import CRF\n",
    "import torch.nn.functional as F\n",
    "import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "# Function to extract noun phrases from POS tags\n",
    "# def extract_noun_phrases(pos_tags):\n",
    "#     # Define patterns for noun phrases\n",
    "#     patterns = [\n",
    "#         [['NN'], ['NNS'], ['NNP'], ['NNPS']],\n",
    "#         [['DT', 'NN'], ['DT', 'NNS'], ['DT', 'NNP'], ['DT', 'NNPS'], ['JJ', 'NN'], ['JJ', 'NNS'], ['JJ', 'NNP'], ['JJ', 'NNPS']],\n",
    "#         [['DT', 'JJ', 'NN'], ['DT', 'JJ', 'NNS'], ['DT', 'JJ', 'NNP'], ['DT', 'JJ', 'NNPS']],\n",
    "#     ]\n",
    "\n",
    "#     noun_phrases = []\n",
    "\n",
    "#     # Iterate through the POS tags\n",
    "#     for i in range(len(pos_tags)):\n",
    "#         # Initialize flag to check if a longer keyword is found\n",
    "#         longer_keyword_found = False\n",
    "\n",
    "#         # Iterate through all possible lengths (up to trigrams)\n",
    "#         for length in range(3, 0, -1):  # Start from length 3 and move to length 1\n",
    "#             if i + length <= len(pos_tags):\n",
    "#                 current_sequence = [tag for word, tag in pos_tags[i:i+length]]\n",
    "#                 current_keyword = ' '.join(word for word, tag in pos_tags[i:i+length])\n",
    "\n",
    "#                 # Check if current sequence matches any pattern\n",
    "#                 for pattern in patterns[length - 1]:  # Adjust index for patterns\n",
    "#                     if current_sequence == pattern:\n",
    "#                         noun_phrases.append(current_keyword)\n",
    "#                         # longer_keyword_found = True\n",
    "#                         # break\n",
    "\n",
    "#                 # if longer_keyword_found:\n",
    "#                 #     break  # Break loop if a longer keyword is found\n",
    "    \n",
    "#     # Sort noun phrases based on their length in reverse order\n",
    "#     noun_phrases.sort(key=len, reverse=True)\n",
    "#     return noun_phrases\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_noun_phrases(pos_tags):\n",
    "    # Define patterns for noun phrases\n",
    "    patterns = [\n",
    "        r'NN.?',\n",
    "        r'DT NN.?',\n",
    "        r'JJ NN.?',\n",
    "        r'DT JJ NN.?',\n",
    "        r'NN.? IN NN.?',\n",
    "        r'NN.? VB.? NN.?',\n",
    "        r'NN.? VBP NN.?|NN.? VBP NNS.?|NNS.? VBP NN.?|NNS.? VBP NNS.?'\n",
    "    ]\n",
    "\n",
    "    noun_phrases = []\n",
    "\n",
    "    # Convert the list of tuples to a space-separated string\n",
    "    pos_string = ' '.join(tag for word, tag in pos_tags)\n",
    "\n",
    "    # Check if the POS string matches any pattern\n",
    "    for pattern in patterns:\n",
    "        for match in re.finditer(pattern, pos_string):\n",
    "            # Get the start and end indices of the match\n",
    "            start, end = match.span()\n",
    "\n",
    "            # Convert the indices to word indices\n",
    "            start = pos_string[:start].count(' ')\n",
    "            end = pos_string[:end].count(' ')\n",
    "\n",
    "            # Extract the corresponding words\n",
    "            noun_phrase = ' '.join(word for word, tag in pos_tags[start:end])\n",
    "            \n",
    "            # Check if the noun phrase is not empty\n",
    "            if noun_phrase.strip():\n",
    "                noun_phrases.append(noun_phrase)\n",
    "\n",
    "    # Remove empty strings from the list\n",
    "    noun_phrases = [phrase for phrase in noun_phrases if phrase]\n",
    "\n",
    "    return noun_phrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oxidation', 'behavior', 'barrier', 'use', 'demand', 'service', 'temperature', 'temperature', 'limit', 'study', 'role', 'composition', 'oxidation', 'behavior', 'attempt', 'limitation', 'alloys', 'production', 'oxidation', 'resistance', 'development', 'pre-oxidation', 'oxidation', 'behavior', 'oxidation', 'rate', 'law', 'depth', 'oxygen', 'ingress', 'thickness', 'number', 'oxidation', 'condition', 'range', 'literature', ']', 'The demand', 'the service', 'these', 'the role', 'that composition', 'the oxidation', 'The attempt', 'this limitation', 'the production', 'the oxidation', 'the literature', 'major barrier', 'Ti-based', 'structural', 'typical temperature', 'careful study', 'Ti-based', 'Ti-based alloys', 'scale thickness', 'limited number', 'certain oxidation', 'compositional range', 'numerous', 'expected', 'the major barrier', 'the typical temperature', 'a limited number', 'a certain oxidation', 'the expected', 'production of', 'development of', 'depth of oxygen', 'number of']\n"
     ]
    }
   ],
   "source": [
    "text = \"Poor oxidation behavior is the major barrier to the increased use of Ti-based alloys in high-temperature structural applications. The demand to increase the service temperature of these alloys beyond 550°C (the typical temperature limit) requires careful study to understand the role that composition has on the oxidation behavior of Ti-based alloys [1–3]. The attempt to overcome this limitation in Ti-based alloys has led to the production of alloys with substantially improved oxidation resistance such as β-21S and also development of coatings and pre-oxidation techniques [1,4–6]. While it is tempting to extrapolate the oxidation behavior (e.g. oxidation rate law, depth of oxygen ingress and scale thickness) observed for a limited number of compositions under a certain oxidation condition to a broader compositional range, there are numerous examples in the literature where deviations from the expected relations are observed [7,8].\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "noun_phrases = extract_noun_phrases(pos_tags)\n",
    "print(noun_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "        # iterate through the files in the data directory\n",
    "        self.txtfiles = []\n",
    "        self.annfiles = []\n",
    "\n",
    "        for file in os.listdir(data_dir):\n",
    "            if file.endswith(\".txt\"):\n",
    "                self.txtfiles.append(file)\n",
    "        \n",
    "        self.tokeniser = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    def tokenise(self , text):\n",
    "        tokens = []  # List to store tokens\n",
    "        starting_offsets = []  # List to store starting offsets\n",
    "        current_token = ''  # Variable to store current token\n",
    "        offset = 0  # Starting offset\n",
    "\n",
    "        for char in text:\n",
    "            if char == ' ':\n",
    "                if current_token:  # If token is not empty\n",
    "                    tokens.append(current_token.lower())  # Append token in lowercase\n",
    "                    starting_offsets.append(offset - len(current_token))  # Store starting offset\n",
    "                    current_token = ''  # Reset current token\n",
    "                offset += 1  # Move offset to next character\n",
    "            else:\n",
    "                current_token += char  # Append character to current token\n",
    "                offset += 1  # Move offset to next character\n",
    "\n",
    "        # Handling the last token if it exists after the loop ends\n",
    "        if current_token:\n",
    "            tokens.append(current_token.lower())  # Append token in lowercase\n",
    "            starting_offsets.append(offset - len(current_token))  # Store starting offset\n",
    "\n",
    "        return starting_offsets , tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.txtfiles)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        txtfile = self.txtfiles[index]\n",
    "        sampleid = txtfile.split(\".\")[0]\n",
    "        \n",
    "        # read the text file\n",
    "        with open(os.path.join(self.data_dir, txtfile), 'r') as file:\n",
    "            txt = file.read()\n",
    "        \n",
    "        # read the annotation file\n",
    "        annfilename = sampleid + \".ann\"\n",
    "        with open(os.path.join(self.data_dir, annfilename), 'r') as file:\n",
    "            ann = file.read()\n",
    "        \n",
    "        offsets , tokenisedtxt = self.tokenise(txt)\n",
    "        tagslist = np.zeros(len(tokenisedtxt))\n",
    "        # now iterate through the ann file , in each line , divide into spaces and get the last word \n",
    "        # make tagslist[i] = 1 if the word is in the tokenisedtxt\n",
    "        for line in ann.split('\\n'):\n",
    "            if line == '':\n",
    "                continue\n",
    "            words = line.split()\n",
    "            if words[0][0] != 'T':\n",
    "                continue\n",
    "\n",
    "            ssofset = words[2]\n",
    "            endoffset = words[3]\n",
    "\n",
    "            # add a 1 to each index of tagslist for indexes where offset is between ssofset and endoffset (including both)\n",
    "            for i in range(len(offsets)):\n",
    "                if offsets[i] >= int(ssofset) and offsets[i] <= int(endoffset):\n",
    "                    tagslist[i] = 1\n",
    "        \n",
    "        # Convert tokens to IDs using BERT tokenizer\n",
    "        tokenisedids = self.tokeniser.convert_tokens_to_ids(tokenisedtxt)\n",
    "        \n",
    "        # Perform POS tagging\n",
    "        pos_tags = nltk.pos_tag(tokenisedtxt)\n",
    "        \n",
    "        # Extract noun phrases from POS tags\n",
    "        noun_phrases = extract_noun_phrases(pos_tags)\n",
    "        \n",
    "        return torch.tensor(tokenisedids), torch.tensor(tagslist), noun_phrases\n",
    "    \n",
    "    def collate_fn(self , batch):\n",
    "        # batch is a list of tuples\n",
    "        # each tuple has 3 tensors , one for tokenisedids, one for tagslist, and one for noun_phrases\n",
    "        # we need to return a tensor of tokenisedids, a tensor of tagslist, and a list of lists for noun_phrases\n",
    "        tokenisedids = []\n",
    "        tagslist = []\n",
    "        noun_phrases = []\n",
    "        for tup in batch:\n",
    "            tokenisedids.append(torch.tensor(tup[0]))\n",
    "            tagslist.append(torch.tensor(tup[1]))\n",
    "            noun_phrases.append(tup[2])\n",
    "        \n",
    "        tokenisedids = torch.nn.utils.rnn.pad_sequence(tokenisedids , batch_first=True , padding_value=0) \n",
    "        tagslist = torch.nn.utils.rnn.pad_sequence(tagslist , batch_first=True , padding_value=0)\n",
    "\n",
    "        tokenisedids = tokenisedids.type(torch.LongTensor)\n",
    "        tagslist = tagslist.type(torch.LongTensor)\n",
    "        \n",
    "        return tokenisedids , tagslist, noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset('../../scienceie2017_train/new')\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True , collate_fn=train_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3532, 19577,  5248,  2003,  1996,  2350,  8803,  2000,  1996,  3445,\n",
       "          2224,  1997,   100, 28655,  1999,   100,  8332,   100,  1996,  5157,\n",
       "          2000,  3623,  1996,  2326,  4860,  1997,  2122, 28655,  3458,   100,\n",
       "           100,  5171,  4860,   100,  5942,  6176,  2817,  2000,  3305,  1996,\n",
       "          2535,  2008,  5512,  2038,  2006,  1996, 19577,  5248,  1997,   100,\n",
       "         28655,   100,  1996,  3535,  2000,  9462,  2023, 22718,  1999,   100,\n",
       "         28655,  2038,  2419,  2000,  1996,  2537,  1997, 28655,  2007, 12381,\n",
       "          5301, 19577,  5012,  2107,  2004,   100,  1998,  2036,  2458,  1997,\n",
       "           100,  1998,   100,  5461,   100,  2096,  2009,  2003, 23421,  2000,\n",
       "           100,  1996, 19577,  5248,   100, 19577,  3446,   100,  5995,  1997,\n",
       "          7722,   100,  1998,  4094,   100,  5159,  2005,  1037,  3132,  2193,\n",
       "          1997,  9265,  2104,  1037,  3056, 19577,  4650,  2000,  1037, 12368,\n",
       "           100,   100,  2045,  2024,  3365,  4973,  1999,  1996,  3906,  2073,\n",
       "           100,  2013,  1996,  3517,  4262,  2024,  5159,   100]),\n",
       " tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
       "         1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64),\n",
       " ['oxidation',\n",
       "  'behavior',\n",
       "  'barrier',\n",
       "  'use',\n",
       "  'demand',\n",
       "  'service',\n",
       "  'temperature',\n",
       "  'temperature',\n",
       "  'limit)',\n",
       "  'study',\n",
       "  'role',\n",
       "  'composition',\n",
       "  'oxidation',\n",
       "  'behavior',\n",
       "  'attempt',\n",
       "  'limitation',\n",
       "  'alloys',\n",
       "  'production',\n",
       "  'oxidation',\n",
       "  'resistance',\n",
       "  'development',\n",
       "  'pre-oxidation',\n",
       "  'oxidation',\n",
       "  'behavior',\n",
       "  'oxidation',\n",
       "  'rate',\n",
       "  'depth',\n",
       "  'oxygen',\n",
       "  'ingress',\n",
       "  'scale',\n",
       "  'thickness)',\n",
       "  'number',\n",
       "  'oxidation',\n",
       "  'condition',\n",
       "  'range,',\n",
       "  'literature',\n",
       "  'the demand',\n",
       "  'the service',\n",
       "  'these',\n",
       "  'the role',\n",
       "  'that composition',\n",
       "  'the oxidation',\n",
       "  'the attempt',\n",
       "  'this limitation',\n",
       "  'the production',\n",
       "  'the oxidation',\n",
       "  'the literature',\n",
       "  'poor oxidation',\n",
       "  'major barrier',\n",
       "  'ti-based',\n",
       "  'typical temperature',\n",
       "  'careful study',\n",
       "  'ti-based',\n",
       "  'ti-based alloys',\n",
       "  '(e.g. oxidation',\n",
       "  'law, depth',\n",
       "  'limited number',\n",
       "  'certain oxidation',\n",
       "  'compositional range,',\n",
       "  'numerous',\n",
       "  'expected',\n",
       "  'the major barrier',\n",
       "  'a limited number',\n",
       "  'a certain oxidation',\n",
       "  'the expected',\n",
       "  'production of',\n",
       "  'development of',\n",
       "  'depth of oxygen',\n",
       "  'number of'])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "Tokenized Text: tensor([ 2195, 28256,   100,   100,  2164,   100,   100,   100,  1998,  2019,\n",
      "         7554,   100,  4681,   100,  2020,  2109,  2000,  7438,  1996, 28269,\n",
      "         2013,  4968,   100,  1998, 22468,  8119,  1999,  2023,   100,  1996,\n",
      "         7812,  4082,  3785,  2020, 12754,  2011,  2309,  5387,  7551,  1998,\n",
      "        28721,  2640,   100,  1998,  1996,  7812,  4082,  3785,  2024,   100,\n",
      "         1996, 13004,  1997,   100,  1998, 14089,  2003,   100,  1998,   100,\n",
      "         4414,  2043,  4363,  1996,  6887,   100,  1998,  1996,  7978,  8208,\n",
      "         3446,  2003,   100,  1998,   100,  2005, 19429,  1998,   100,  2241,\n",
      "         2006,  1996,  6388,   100,  2023,  3259,   100,  1996,  2364,  5876,\n",
      "         2008,  7461, 28269,   100,   100])\n",
      "Tags List: tensor([0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        0., 0., 1., 1., 1.], dtype=torch.float64)\n",
      "Noun Phrases: ['al2(so4)3,', 'fecl3', 'coagulant', 'aid', 'pam,', 'wastewater', 'anima', 'poultry', 'breeding', 'paper.', 'ideal', 'factor', 'experiment', 'design', 'experiment.', 'ideal', 'operating', 'dose', 'feso4', 'pam', 'ph', 'removal', 'rate', 'cod', 'turbidity.', 'results,', 'paper', 'flocculation', 'the wastewater', 'this paper.', 'the ideal', 'the ideal', 'the dose', 'the ph', 'this paper', 'feso4, al2(so4)3,', 'organic coagulant', 'domestic anima', 'single factor', 'orthogonal design', 'corresponding removal', 'experimental results,', 'main', 'an organic coagulant', 'the corresponding removal', 'the experimental results,', 'the main', 'dose of feso4', 'ideal operating']\n",
      "\n",
      "Sample 2:\n",
      "Tokenized Text: tensor([  100,   100,  2003,  2028,  1997,  3239,  2929, 10903,   100,  1037,\n",
      "         3145,  3291,  1997,   100,  2003,  2000, 14125, 10197,  1996, 11136,\n",
      "          100,  2059,  1037, 11136,  3295,  4118,  2241,  2006, 19476,  1998,\n",
      "          100,  9896,  2001,  3818,  2005,  1037,   100,   100,  2291,  2029,\n",
      "         2001,  2764,  2256,  6745,   100,   100,  1037,  7965,  6951,  2001,\n",
      "         3107,  2000,  2079,   100,  9651,  3231,  2000, 16157,  1996, 11136,\n",
      "         3295,   100,  6388,  3463,  3662,  2008,  1996,  4118,  2071,  2092,\n",
      "         7637,  3239,  2929,  1998,  3113,  1996, 11436,  3463,  2007,   100])\n",
      "Tags List: tensor([1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
      "        1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 1.], dtype=torch.float64)\n",
      "Noun Phrases: ['video-oculography', '(vog)', 'eye', 'movement', 'measurement', 'problem', 'vog', 'pupil', 'center.', 'pupil', 'location', 'method', 'morphology', 'canny', 'algorithm', 'vog', 'system', 'work.', 'moreover,', 'volunteer', 'test', 'pupil', 'location', 'method', 'eye', 'movement', 'the pupil', 'a pupil', 'the pupil', 'the method', 'key problem', 'wifi-based vog', 'healthy volunteer', 'experimental', 'anticipated', 'a key problem', 'a wifi-based vog', 'a healthy volunteer', 'the anticipated', 'problem of vog', 'results with']\n",
      "\n",
      "Sample 3:\n",
      "Tokenized Text: tensor([  100,  2064,  2022,  3141,  2000,  2060,   100,  6228,  5144,  2107,\n",
      "         2004, 10750,   100,   100,  1998, 21274,   100,   100,  2119,  5173,\n",
      "         2013, 13379,   100,  2005,   100,   100,   100,  5173,  1037,  3622,\n",
      "          100,  2090, 23608,  1998,   100, 10750,   100,   100,   100,   100,\n",
      "         2009,  2001,  2574,  3651,  2008,   100,  3276,  2069, 12033,  2000,\n",
      "         4475,  2008,  8327,  2440,   100,   100,   100,  2013,  2023,  3276,\n",
      "         2031,  2042,  2988,  2005,  1037,  2193,  1997,   100,  7877,  1998,\n",
      "        27216,  2073,  1996, 21274, 18859,  2024,   100,   100,   100,  1996,\n",
      "         2367, 11423,  7851,  1996, 16902,  1997, 23608,  2007,  7511,   100,\n",
      "         6228,  5144, 11160,  2006,  1996, 16406,  1997,  1996,   100,   100,\n",
      "          100,  1999,  2023,   100, 23608,  1998, 10750,  6911,  2053,  2936,\n",
      "         2907,  3622,   100,  2021,  2037,  3276,  9041,  2006,  1996,  3563,\n",
      "         3430,   100,  2107,  2004,   100,  6463,  1998, 21274,   100,   100,\n",
      "         2009,  2038,  2042,  3491,  2008,  2122,   100,  4275,  2025,  2069,\n",
      "          100,  4863,  2019,   100,  6463,  1997,   100,  2005,  1037,  2193,\n",
      "         1997,   100,  4475,  1997,  2367,   100,  2021,  2036, 22634,  4070,\n",
      "         2005,  1996,  2846,  1997,   100, 21879,   100,  4340,   100])\n",
      "Tags List: tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
      "        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64)\n",
      "Noun Phrases: ['microhardness', 'yield', 'stress,', 'modulus,', 'compression', 'testing.', 'metals,', 'tabor', 'proportionality', 'hardness', 'yield', 'stress:', 'h≈3σ', \"tabor's\", 'relationship', 'plasticity', 'relationship', 'number', 'correlation', 'hardness', 'validity', 'models.', 'way,', 'hardness', 'yield', 'stress', 'proportionality', 'relationship', 'material', 'properties,', 'ratio', 'modulus', '[9,11–13].', 'h/σ', 'ratio', '≈2', 'number', 'range', 'this relationship', 'a number', 'the correlation', 'the validity', 'this way,', 'an h/σ', 'a number', 'the range', 'mechanical', 'elastic modulus,', 'work-hardened metals,', 'direct proportionality', 'compressive yield', 'full plasticity', 'metals,', 'elastic', 'non-negligible', 'different', 'mechanical', 'elasto-plastic models.', 'direct proportionality', 'specific material', 'elastic modulus', 'elasto-plastic', 'polyethylene', 'different', 'h/e', 'a direct proportionality', 'the elastic', 'the different', 'the specific material', 'these elasto-plastic', 'proportionality between hardness', 'correlation of hardness', 'ratio of ≈2', 'plasticity [9,10].', 'plasticity [9,10].']\n",
      "\n",
      "Sample 4:\n",
      "Tokenized Text: tensor([ 2057,  5136,   100,  2925,   100,  2030,  2502, 10973,   100,   100,\n",
      "         2029,  2089,  5258,  2130,  2043,  2844,  2943,  4650,  2003,  2025,\n",
      "        14424,  2021,  8522,  1997,  2110, 16381,  2003,   100,   100,  2742,\n",
      "         1997,  2107,   100,  2038,  2042,  3591,  2011,   100,  2057,  2179,\n",
      "         2178,  2742,  1997,   100,  2635,  2046,  4070,  2067,  4668,  1997,\n",
      "          100,  8559,  4249,  2379,   100,  2009,  2003,  3491, 12045,  2008,\n",
      "         8559,  3896,  2089,  8536,   100,  2191,   100,  1996,   100,  2009,\n",
      "         2003,  5275,  2008,  2065,  1996,  6622,  2000,   100,  2003,   100,\n",
      "         2349,  2000,  8559,  3896,  1996,  5304,  2089,  2203,  2039,  1999,\n",
      "         2139,   100,  4403,  2077,  4094,  5387, 13783,   100,  2023,  3861,\n",
      "         2003, 18960,  2005,   100,  2073,  5573,   100,  2089,  5258,  2006,\n",
      "         1996,   100,  2007,   100,  2714,   100])\n",
      "Tags List: tensor([0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 1., 1., 1.], dtype=torch.float64)\n",
      "Noun Phrases: ['future', '(sudden', 'rip', 'type)', 'energy', 'condition', 'equation', 'state', 'parameter', 'example', 'singularity', 'barrow,', 'example', 'it.', 'account', 'reaction', 'quantum', 'singularity,', 'milder)', 'singularity.', 'evolution', 'singularity', 'universe', 'phase', 'factor', 'picture', 'braneworld', 'singularity', 'brane', 'another example', 'the singularity.', 'the evolution', 'the universe', 'this picture', 'the brane', 'finite-time, future', 'big rip', 'strong energy', 'recently, example', 'such singularity', 'conformal quantum', 'quantum', 'scale factor', 'sudden singularity', 'similar', 'equation of state', 'example of it.', 'fields near singularity,', '(or make milder)']\n",
      "\n",
      "Sample 5:\n",
      "Tokenized Text: tensor([ 4517,  3399,  7422,  2350,  4073,  2144,  1018,  5109,  2000,  6235,\n",
      "          100,  1999,  4517,   100,  9197,  2478,   100,   100,   100,  1999,\n",
      "         2240,  2007,  2714,  3471,  1999,  8559,   100,   100,  2045,  2020,\n",
      "         4740,  2000,  4503,  5301,  8382, 10949,  4725, 11566,  8559,  2838,\n",
      "         2007,  1037,  4100,  4556,  3949,  1997,   100,   100,   100,   100,\n",
      "         2053,   100,  8559,  3921,  2003, 12192,  2800,   100,  1999,  8741,\n",
      "         1997,  3365,  5337,  4740,   100,  1996,  2492,  1997, 12906,  1998,\n",
      "        28991,  5090,  2003,  2521,  3920,  2021,  3435,  4975,  1999,  7189,\n",
      "         2000,  1996,  7552,  8973,  1997, 23965,  1998, 12126,   100,   100,\n",
      "         8107,  2020,  2036,  2641,  1999,  1996,  2492,  2000,  2421,  2070,\n",
      "          100,   100,   100,  1998,  2071,   100,  6235,   100,   100,  2021,\n",
      "         2107,  8107,  2024,  5391,  2000,  3722, 11970,  2007, 12949,   100,\n",
      "         4400,   100,  1998,  2947,  5744,   100,   100,   100,   100,  1996,\n",
      "         2553,  1997,  7554,   100,  1999,  3327,  1996,  2172,  6334,   100,\n",
      "          100,   100,  3685,  2022,  5845,  2023,   100,  4100,   100,  1998,\n",
      "         2130,  4556,   100,  2064,  2022,  2109,  2012,  2200,  2152,   100,\n",
      "         2107,  2004,  5359,  2011,  2200,  6387,  9138,   100,   100,  1999,\n",
      "         2107,  3572,  1996,  2291,  2003, 10676,  2039,  1998,  4751,  1997,\n",
      "         2049,  8559,  6228,  2838,  2079,  2025,  3043,   100,  2021,  2005,\n",
      "         2625,  6355,   100,  8559,  5806,  3896,  3685,  2022,   100])\n",
      "Tags List: tensor([1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "        0.], dtype=torch.float64)\n",
      "Noun Phrases: ['theory', 'thermalization', 'reactions,', 'methods\\xa0', '[13,14,10],', 'line', 'liquids\\xa0', '[15,16].', 'quantum', 'treatment', 'correlations\\xa0', 'quantum', 'approach', 'yet,', 'spite', 'field', 'developing', 'relation', 'field', 'corrections\\xa0', '[21,22]', 'processes.', 'approximations.', 'case', 'systems,', 'c60', 'cannot', 'way.', 'semi', 'classical,', 'approaches,', 'laser', 'pulses\\xa0', '[2].', 'system', 'scenarios,', 'quantum', 'shell', 'the field', 'the field', 'the case', 'this way.', 'the system', 'nuclear theory', 'major', 'nuclear reactions,', 'semi-classical methods\\xa0', 'similar', 'quantum liquids\\xa0', 'molecular', 'classical treatment', 'dynamical correlations\\xa0', 'clear-cut quantum', 'available yet,', 'formal', 'nano', 'fast developing', 'ongoing', 'semiclassical', 'dynamical corrections\\xa0', 'dynamical processes.', 'such', 'simple', 'smooth', 'semiclassical approximations.', 'organic systems,', 'classical approaches,', 'high', 'intense laser', 'such', 'mechanical', 'violent scenarios,', 'no clear-cut quantum', 'the ongoing', 'some dynamical corrections\\xa0', '[13,14,10], in line', 'yet, in spite', 'field of', 'developing in relation', 'developments of', 'methods combining quantum']\n",
      "\n",
      "Sample 6:\n",
      "Tokenized Text: tensor([ 1996,  3001,  1999,  2029,  1996,   100,  2486,  2003,  2087,  4069,\n",
      "         2024,  2216,  2007,  1037,  2152, 17225,  2492,   100,  2930,  1016,\n",
      "        10592,  1996, 13494,  1997,  1996, 19780,  2090,  1996,  6714,  1997,\n",
      "         1037,  4556, 10496,  1998,  1996,  5901,  9671, 17225,  2492,  2550,\n",
      "         2011,  1037,   100, 12123,   100, 12949,   100,   100,  9138, 23894,\n",
      "         2064,  2433, 20134,  5975,  2306,  1996, 10496,  4304,  1997,  1037,\n",
      "          100,  2122,  4304,  5975,   100,  2007,  3177, 12435,  2000,  1996,\n",
      "         2177,  3177,  1997,  1996,  9138,   100,  2025,  2035, 12123, 15057,\n",
      "         2433,  2023,   100,   100,  2070,  1997,  1996, 15057,  2024,  3236,\n",
      "         2039,  1999,  1996,  4400,  1998, 14613,  2011,  2049,  2152,   100,\n",
      "         1996,  4400,  2776, 25938,  2004,  2122, 15057, 10620,  1996,  4400,\n",
      "          100,  4400,   100,  1996,  5186,  2152,  3751,  2492, 17978,  1997,\n",
      "         1037, 12123,  4400,  2379,   100,  3640,  2019,  6581,  9373,  5604,\n",
      "         2598,  2005,  1996,  3896,  1997,   100,  5857,  2000,  1996, 22793,\n",
      "         1997,  1037,  3231,   100])\n",
      "Tags List: tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       dtype=torch.float64)\n",
      "Noun Phrases: ['stern–gerlach', 'force', 'field', 'gradient.', 'section', 'coupling', 'spin', 'electron', 'field', 'plasma', 'wave.', 'high-intensity', 'laser', 'density', 'plasma.', 'density', 'speed', 'group', 'speed', 'laser', 'pulse.', 'wave,', 'wave', 'fields.', 'wave', 'wave', 'wave', 'field', 'gradient', 'plasma', 'wave', 'testing', 'ground', 'trajectory', 'test', 'the', 'the stern–gerlach', 'the', 'the coupling', 'the spin', 'a plasma.', 'these density', 'the group', 'the laser', 'this wave,', 'the', 'the wave', 'the wave', 'these', 'the wave', 'a plasma', 'the', 'the trajectory', 'a test', 'electromagnetic field', 'classical electron', 'electromagnetic field', 'laser-driven plasma', 'longitudinal', 'electron density', 'plasma', 'high fields.', 'electric field', 'theoretical testing', 'stern–gerlach-type', 'a classical electron', 'a laser-driven plasma', 'the electron density', 'all plasma']\n",
      "\n",
      "Sample 7:\n",
      "Tokenized Text: tensor([ 1999,   100,  2057,  2031,  2764,  1037,  6028,  2005,   100,   100,\n",
      "         2946,  7312,  2011,   100,   100,  6726, 10496, 24635,  4871,  1997,\n",
      "         1037,   100,   100, 12490,   100,  2007,  9381,  4359,  2013,  3770,\n",
      "         2000,   100,  2265,  6375, 28375,  2247,  1996,  3091,  1997,  1996,\n",
      "         7318,  1998,  2152,  5813,  4871,  2265,  2053,  5793,  3431,  1997,\n",
      "         1996, 19476,  2044,   100,  1996,  4187,  2783,  4304,  1997,  1996,\n",
      "          100,  7318,  1998,  2028,   100,  2000,  1037,  9381,  1997,   100,\n",
      "         2003,   100,  1998,   100,  2012,   100,   100,  9104, 27018, 25502,\n",
      "         1997,  1996,  5992,  5144,  2076,   100,  2122,  3463,  6592,  2008,\n",
      "          100,  2003,  1037,  4022,  3921,  2005,   100,  2946,  7312,  2007,\n",
      "         2152,  5813,  2875,  1996,  8089,  1997,   100,  1998,  8559,   100,\n",
      "         2004,  2092,  2004,  2005,  2810,  1997,  7605,   100,   100])\n",
      "Tags List: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "        1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "Noun Phrases: ['summary,', 'technique', 'nanowire', 'size', 'reduction', 'transmission', 'microscope', 'nanowire', 'width', 'show', 'shrinking', 'length', 'wire', 'resolution', 'morphology', 'density', 'wire', 'width', 'respectively,', 'modulation', 'fib-milling', 'approach', 'size', 'reduction', 'resolution', 'observation', 'effects,', 'construction', 'a technique', 'the length', 'the wire', 'the morphology', 'a width', 'these', 'the observation', 'site-specific nanowire', 'thinning. transmission', 'composite nanowire', 'uniform shrinking', 'high resolution', 'obvious', 'current density', 'as-deposited wire', 'insignificant modulation', 'electrical', 'potential approach', 'controllable size', 'high resolution', 'quantum effects,', 'no obvious', 'the as-deposited wire', 'the electrical', 'a potential approach', 'transmission electron microscope', 'nanowire with width']\n",
      "\n",
      "Sample 8:\n",
      "Tokenized Text: tensor([ 2104,  2122,  6388,   100,  1996,  5159, 10949,  2038,  2000,  5258,\n",
      "         2073,  1996, 15113,  9138,   100,  1996,  9597,  4525,  1999,  2582,\n",
      "          100,   100,  1996,   100, 13121,  2944,   100,  2001,  4162,  2000,\n",
      "         4863,  1996,   100, 28424,  1997,   100,  2000,   100,  3491,  1999,\n",
      "         3275,   100,  1996, 11414,  1997,  1996,  4125,  1998, 13121,  6177,\n",
      "         1997,  1996,   100,  2020,  2589,  2011,   100,  4730,  2478,  1996,\n",
      "         7774, 11414,   100,   100,  1996,  2190,  4906, 13121,   100,  2005,\n",
      "         1996,   100, 13121,  6177,  1997,   100, 10163,  4742,  2003,   100,\n",
      "         1998,   100,  2096,  2008,  2005,   100, 10163,  4742,  2003,   100,\n",
      "         1998,   100,   100,  2122, 13121,   100, 23758,  2000,  1996,  3130,\n",
      "         2988,  2051,   100,  1997,   100,  1998,   100,   100,  1996,   100,\n",
      "         1997,  1996,  4668, 15778,   100,  2024, 12949,  2367,  2013,  2008,\n",
      "         1997,  1996,  6687, 10163,   100,  8131,  2008,  2057,  2024,  5702,\n",
      "         1996,  5664, 10949,  1997,  1996,   100,  1998,  2025,  2008,  1997,\n",
      "         1996,  6687, 10163, 28424,   100, 11243,  9138,  2491,  6481,  2104,\n",
      "         2107,  6388,  6214,  2036, 23283,  2008,  2057,  2024,  9756,  1996,\n",
      "         4031, 10750,  1997,   100,  4525,  2013,  1996,   100,  4668,  1997,\n",
      "          100])\n",
      "Tags List: tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
      "        0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
      "        1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
      "        0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "        0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "Noun Phrases: ['conditions,', 'probe', 'laser', 'ionization', 'decay', 'model', 'fragmentation', 'dcpd', 'figure', 'fitting', 'rise', 'matlab®', 'curve', 'decay', 'decay', 'ion', 'signal', 'c5h6+', 'ion', 'signal', 'respectively.', 'decay', 'time', 'norbornene', 'reaction', 'fragment', 'c5h6+', 'parent', 'ion', 'c10h12+', 'parent', 'ion', 'fragmentation', 'laser', 'control', 'product', 'yield', 'c5h6+,', 'reaction', 'the probe', 'the', 'the fitting', 'the rise', 'the', 'the curve', 'these decay', 'the', 'the reaction', 'the parent', 'the', 'the parent', 'the product', 'experimental conditions,', 'observed', 'further ionization', 'two-step decay', 'above-mentioned fragmentation', 'decay', 'fit decay', 'biexponential decay', 'c10h12+ ion', 'τ2=280fs, respectively.', 'distinct', 'experimental', 'photochemical reaction', 'these experimental conditions,', 'the observed', 'the two-step decay', 'the above-mentioned fragmentation', 'the biexponential decay', 'the distinct', 'the photochemical reaction', 'fragmentation of dcpd', 'constants of norbornene', 'yield of c5h6+,', 'reaction of', '[24]. applying laser']\n",
      "\n",
      "Sample 9:\n",
      "Tokenized Text: tensor([  100, 16268,  2031, 20485,  1996,  4816,  3252,  1998,  9211,  1997,\n",
      "         1996,   100,  9324,  2104,   100, 13922,   100,  1998,   100,  2256,\n",
      "         3463,  2265,  2008,  1996,   100,   100,   100, 26113,  7980,  2003,\n",
      "         2200,   100,  2000,   100,  1996,   100, 13922,  2069, 17541,   100,\n",
      "         3031,  1996,  9324,  2012,  4659,  2659,   100,  1999,  1996,  2846,\n",
      "         1997,  2195, 15295,   100,   100,  3278,  4668, 13500,   100,   100,\n",
      "         2005,  1996,   100, 13922,  2006,  1996,  9324,  2024,  4453,  2006,\n",
      "         2367,   100,   100,   100,  2122,  4668, 10425,  2024,  6714, 10386,\n",
      "         9597,  2429,  2000,   100,  6714,  4989,   100,  2122,  3463, 19515,\n",
      "         2008,   100, 12192,   100,  2013,  1996,  9324,  3302,  2738,  2084,\n",
      "          100,  1998, 15772,  1996,   100,  9324,  2588,   100,  1999,  2152,\n",
      "         4860,  1998,  2152,  3778,   100,  1996,   100, 10737,  2089,   100,\n",
      "         2006,  1996,   100,  3341,  2609,  2011, 27363,  1037,  6022,  2312,\n",
      "         2943,   100])\n",
      "Tags List: tensor([1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "        1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "Noun Phrases: ['structure', 'stability', 'w@si12', 'cluster', 'o2', 'molecule', 'adsorption', 'prism', 'cage', 'o2', 'molecule', 'cluster', 'temperatures,', 'range', 'reaction', 'ev)', 'molecule', 'cluster', 'adsorption', 'sites,', 'reaction', 'selection', 'cluster', 'surface', 'w@si12', 'cluster', 'excitations.', 'temperature', 'pressure', 'edge', 'site', 'energy', 'the w@si12', 'the o2', 'the cluster', 'the range', 'the cluster', 'these reaction', 'these', 'the cluster', 'the w@si12', 'first-principles', 'electronic structure', 'hexagonal prism', 'low temperatures,', 'several', 'significant reaction', 'o2 molecule', 'different adsorption', 'forbidden', 'spin selection', 'high temperature', 'high pressure', 'o2', 'preferential edge', 'large energy', 'the electronic structure', 'the o2 molecule', 'the o2', 'the preferential edge', 'cluster under o2', 'cluster upon excitations.', 'barriers (0.593–1.118 ev)', 'barriers (0.593–1.118 ev)']\n",
      "\n",
      "Sample 10:\n",
      "Tokenized Text: tensor([ 1996, 15772, 14983,  2003, 10174,  2478,  1996,  3635,  5114,  1998,\n",
      "         3302,   100,   100,  5278,  1996,  3302,  6337,  2097, 19933,  1996,\n",
      "         3302,  2181,  1998, 10174, 15772,   100,   100,  4871,  1997,  8168,\n",
      "         3718,  2044, 11118,  2420, 19577,  2020,  2109,  2000,  9375,  1996,\n",
      "         2689,  1999,  3302,  6337,  3091,  2007,  8386,  1999,  4162,   100,\n",
      "         1996,  6337, 10742, 15901,  2013,  1996,  4871,  2020,  2059,  2109,\n",
      "         2000, 19933,  1996,  3091,  1997,  1996,  7099,  1998,  3568,  1996,\n",
      "         3302,   100,  2795,  1015,  3065,  1996,  2434, 15772,   100,  2044,\n",
      "        11118,  2420,   100,  1996,  6310, 15772,   100,  2241,  2006,  1996,\n",
      "         3302,  6337,  3091,  1998,  1996,  7017,   100,  3463,  2265,  1037,\n",
      "         4555,  9885,  1999,  1996, 15772, 14983,  1997,   100,  2043,  2478,\n",
      "         1037,  3302,  2029,  6115,  2005,   100, 13599,  1996,  2689,  1999,\n",
      "        15772, 14983,  2090,  2367,  3302, 12321,  7127,  1037,  8386,  1997,\n",
      "         2625,  2084,   100,  2004,   100,  1996,  4254,  1997,  1996,  8386,\n",
      "         1999,  1996,  6337,  3091,  2006,  1996, 10174, 15772, 14983,  2003,\n",
      "         2641,  2000,  2022,   100,  1999,   100,  2065,  1996,  5966,  1999,\n",
      "         3635,  5114,  2020,  2069,  2349,  2000,  5966,  1999,  3302,   100,\n",
      "          100,  8168,  2052,  2022,  3517,  2000, 10580, 19638,   100,  2012,\n",
      "         1996,  5700,  5711,  1997,   100])\n",
      "Tags List: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 1.], dtype=torch.float64)\n",
      "Noun Phrases: ['thickness', 'weight', 'gain', 'surface', 'surface', 'profile', 'surface', 'area', 'sem', 'oxidation', 'change', 'surface', 'profile', 'length', 'variation', 'length', 'sample', 'surface', 'area.', 'table', 'oxide', 'oxide', 'surface', 'profile', 'length', 'percentage', 'difference.', 'decrease', 'thickness', 'surface', 'roughness.', 'change', 'thickness', 'surface', 'variation', 'such,', 'impact', 'variation', 'profile', 'length', 'oxide', 'thickness', 'addition,', 'gain', 'surface', 'area,', 'rougher', 'thicker', 'the weight', 'the surface', 'the surface', 'the change', 'the', 'the length', 'the sample', 'the surface', 'the surface', 'the percentage', 'a surface', 'the change', 'a variation', 'the impact', 'the variation', 'the profile', 'the', 'oxide thickness', 'thickness. sem', 'profile', 'original oxide', 'modified oxide', 'maximum decrease', 'oxide thickness', 'oxide thickness', 'different surface', 'calculated oxide', 'weight gain', 'the oxide thickness', 'the profile', 'the original oxide', 'the modified oxide', 'a maximum decrease', 'the oxide thickness', 'the calculated oxide', 'images of', 'change in surface', 'length with variation', 'differences in surface', 'stages of']\n",
      "\n",
      "Sample 11:\n",
      "Tokenized Text: tensor([ 1999, 11131,  1996,   100,  5787,  1997,  8559,   100,   100,   100,\n",
      "         2001,  1996,  2034,  2000,  5060,  2008,  2348,  2028,  4627,  2007,\n",
      "         2035,  1996,   100,  2055,  1996,  3267,  1997,  1037,  8559,   100,\n",
      "         1996,  2034,  2344, 20167, 16142,  1996,  6623,  4556,   100,  2011,\n",
      "         2008,  2057,  2812,  2008,  1996,  2613,  2112,  1997,  1996,   100,\n",
      "         8522,  2104, 11508, 22511,  1997,  1996,  4400,  3853,  4150,  1996,\n",
      "         4556,   100,  8522,  1999,  1996,  5787,  2073,  3408,  5994,   100,\n",
      "         2024,   100,  1999,  5688,  2000,  2023,   100,  1999,  2023,  3661,\n",
      "         2057,  2265,  2008,  1996,  4556,   100, 13368,  2013,  1037,   100,\n",
      "         8559,   100,  2043,  3408,  1997,   100,  2064,  2022,   100,  2023,\n",
      "         2755,  2001,  2941,  2525,  5159,  2011,  7935,  2070,  3174,  2086,\n",
      "          100,  1999,  3931, 25717,  1997,  2010,  2338,   100,   100,  2051,\n",
      "        14025,  2024,  2641,  3005,  5537,   100,  1037, 10713,   100,  2009,\n",
      "         2003,  3491,  2008,  2247,  2169,  6903,  1996,  4367,  2003,  4556,\n",
      "          100,  8559,   100,  1998,  2008,  2009,  4076,  2008,  1996,  8559,\n",
      "         4130,  2089,  2022,   100,  2046,  1037,  5537,  1997,  9214,  2247,\n",
      "         2169,  1997,  2029,  1996,  4556,  2895,  2003,  1037,   100,  1996,\n",
      "         3117,  6691,  1997,  1996,  2556,  3661,  2003,  2019,  5301,  6947,\n",
      "         1997,   100,  2765,  2478,  2019,  5301,  2544,  1997,  1996,   100,\n",
      "         2349,  2000,   100,  1998,  4679,   100,   100,  2036,  2139,   100,\n",
      "          100,  2005,  1037,  2582,   100])\n",
      "Tags List: tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64)\n",
      "Noun Phrases: ['limit', 'quantum', 'theory,', 'bohm', '[2]', 'nature', 'quantum', 'system,', 'order', 'approximation', 'ontology.', 'part', 'equation', 'decomposition', 'wave', 'function', 'hamilton–jacobi', 'equation', 'limit', 'ℏ', 'contrast', 'approach,', 'letter', 'quantum', 'propagator', 'o(δt2)', 'fact', 'holland', 'page', 'book', 'time', 'sequence', 'path.', 'segment', 'motion', 'quantum', 'potential),', 'path', 'sequence', 'action', 'minimum.', 'contribution', 'letter', 'proof', 'hollandʼs', 'result', 'version', 'propagator', 'gosson', '[3]', 'the', 'the nature', 'a quantum', 'the wave', 'the limit', 'this approach,', 'this letter', 'this fact', 'each segment', 'the motion', 'a sequence', 'a minimum.', 'the propagator', 'wkb limit', 'first order', 'classical ontology.', 'real part', 'schrödinger equation', 'polar decomposition', 'classical hamilton–jacobi', 'classical', 'short-time quantum', 'infinitesimal time', 'finite path.', '(negligible quantum', 'quantum path', 'classical action', 'novel contribution', 'present letter', 'improved proof', 'improved version', 'further', 'the wkb limit', 'the first order', 'the real part', 'the schrödinger equation', 'the classical hamilton–jacobi', 'the classical', 'a short-time quantum', 'a finite path.', 'the quantum path', 'the classical action', 'the novel contribution', 'the present letter', 'an improved proof', 'an improved version', 'a further', 'limit of quantum', 'terms of o(δt2)', 'sequence of', 'proof of hollandʼs', 'terms involving ℏ']\n",
      "\n",
      "Sample 12:\n",
      "Tokenized Text: tensor([ 1996,  2342,  2005,  2373,  4245,  3068,  2000,  5335,  1996,  9829,\n",
      "         8122,  1997,  2373,  3269,  2038,  2419,  2000,  1996,  2458,  1997,\n",
      "          100, 13675,   100,   100,  1996,  2458,  1997,  1998,  2470,  2006,\n",
      "          100,   100,  2318,  2144,  2397,  3955,  1998,  2220,   100,  4414,\n",
      "          100,  1996,  2147,  2038,   100,  2006,  2037, 19815, 20828,  2349,\n",
      "         2000,  2049,  3832,  4646,  2012,  2152,   100,   100,  1996,  4955,\n",
      "         1997,  2062, 23750,  3169,  1997,  2373,  3269,  2038,  3107,  1996,\n",
      "         6061,  1997, 16342,   100,  8501, 15729,  2349,  2000,  1996,  3896,\n",
      "         1997,  9671,  5492, 12959,  2038,  2042,  2988,   100,  1996,  4860,\n",
      "         9670,  5320,  9829,   100,  2090,  1996,  2503,  1998,  2648,  1997,\n",
      "         6177,  1998,  2023,  2064,  3426, 23750,  6911,  3798,  2000,  2022,\n",
      "         1997,   100,   100,  2470,  2006,   100,  4106,  1997,   100,  2038,\n",
      "         2042,  3344,  2041,  2164,  1996,   100,  1997,  1996, 23750,  9164,\n",
      "         1997,  1996,  3430,  2478,  1996,   100,  1998, 10562,   100,  4275,\n",
      "          100])\n",
      "Tags List: tensor([0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.],\n",
      "       dtype=torch.float64)\n",
      "Noun Phrases: ['need', 'power', 'generation', 'industry', 'efficiency', 'power', 'plant', 'development', 'development', 'research', 'p91', 'work', 'application', 'temperature.', 'introduction', 'operation', 'power', 'plant', 'possibility', 'fatigue', 'problems.', 'steam', 'warming', 'temperature', 'cycling', 'inside', 'stress', 'recently,', 'research', 'analysis', 'p91', 'characterisation', 'behaviour', 'material', 'two-layer', 'visco-plasticity', 'the need', 'the development', 'the development', 'the work', 'the introduction', 'the possibility', 'the', 'the temperature', 'the inside', 'the characterisation', 'the material', 'the two-layer', 'thermal efficiency', 'creep', 'high temperature.', 'cyclic operation', 'thermal', 'cyclic stress', 'concerns. recently,', 'thermal–mechanical analysis', 'cyclic behaviour', 'unified visco-plasticity', 'the thermal efficiency', 'the cyclic behaviour', 'need for power', 'efficiency of power', 'research on p91', 'operation of power', 'possibility of fatigue', 'analysis of p91']\n",
      "\n",
      "Sample 13:\n",
      "Tokenized Text: tensor([ 1996,  3739,  1997, 19633,  4367,  1999,  4517,  3001,  2038,  2042,\n",
      "         7933,  3141,  2007,  1996,  6747,  1997,   100,  2943,   100,   100,\n",
      "        20611,  1997,   100,   100,  1997, 11165,  4517,  2030,  9593,  7568,\n",
      "         3798,  2007,  1996,  2168,  6714,  1998,   100, 17254,  2000,   100,\n",
      "         4556,   100,  2096,   100,  6747,  4742, 19633,  4367,  1999,  1996,\n",
      "         7978,  4556,   100,  7783,  8146,  2024,  2062,  3697,  2000,   100,\n",
      "         2200,  3728,  1037,  6378,  2038,  2042,  2081,  2000,  7438,  1996,\n",
      "        17435, 28892,   100,  2004, 16246,  2051,  2186,   100, 12854,   100,\n",
      "         2007,   100,  1996,  2812,  2504,  4304,  2029,  4473,  1996, 12375,\n",
      "         2000,   100,  3798,  2007, 22127,  2779,  2504,   100,  1998, 20253,\n",
      "         1996,  2943, 28892,  2004,  1037, 16246,  2051,   100,  2027,  2179,\n",
      "         2008,  4517,  2373, 29237, 16582,  2066,   100,   100,   100,  2008,\n",
      "         2023,  2453,  2022,  1037,  8281,  8085,  1997, 12391,  8559, 19633,\n",
      "          100,  1999,  1996,  2556,  2147,  2057, 10408,  2023,   100,  2478,\n",
      "         1996,   100, 17435,  5248,  2004,  1037,  3231,  2005,  1996,  3739,\n",
      "         1997,  8488,  1999,  4517,  3742,   100])\n",
      "Tags List: tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
      "        1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
      "        0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "Noun Phrases: ['presence', 'motion', 'energy', 'levels\\xa0[8,9].', 'spin', 'parity', 'correspond', 'dynamics,', 'motion', 'regime\\xa0[10].', 'proposal', 'time', 'series', '(1)δn=∫−∞en+1ρ˜(e)de−n,', 'level', 'density', 'mapping', 'dimensionless', 'level', 'density,', 'energy', 'time', 'series,', 'power', 'signature', 'quantum', 'systems.', 'work', 'idea,', 'behavior', 'test', 'presence', 'chaos', 'mass', 'the presence', 'the', 'a proposal', 'the mapping', 'the energy', 'this idea,', 'a test', 'the presence', 'chaotic motion', 'nuclear', 'high-lying energy', 'poisson', 'normalized', 'excited', 'same spin', 'classical dynamics,', \"wigner's\", 'chaotic motion', 'classical regime\\xa0[10].', 'intermediate', 'spectral', 'discrete time', 'mean level', 'average level', 'discrete time', 'nuclear power', 'characteristic signature', 'generic quantum', 'chaotic systems.', 'present work', 'spectral behavior', 'nuclear mass', 'the same spin', 'the spectral', 'the mean level', 'a discrete time', 'a characteristic signature', 'the present work', 'presence of chaos', '[11]. defining (1)δn=∫−∞en+1ρ˜(e)de−n,']\n",
      "\n",
      "Sample 14:\n",
      "Tokenized Text: tensor([ 4427,  2011,   100, 13352,  2107,  2004,   100,   100,  6223,   100,\n",
      "         2076,  6897,   100,  1996,  3399,  1997,  3161, 11508,   100,   100,\n",
      "         2038,  2042,  2764,   100,  1996,  3399,  4275,  1996,   100,   100,\n",
      "         9760,  1997,  1037,  3074,  1997,   100,  3161,   100, 11157,  1999,\n",
      "         1037,   100,  9625,   100,  1999,  2029,  4722, 23253,  2024, 10572,\n",
      "         2349,  2000,   100,  1997,  2943,   100,  1996,   100,   100, 11508,\n",
      "         6074,   100,  1996, 21500,  2024, 14440,  2004,  3131,   100,  1996,\n",
      "         2779,  1997,  3131, 19019,  1999,  1037,  2235,  2334,  3872,  2012,\n",
      "         2169,  2391, 11859,  1996,   100,   100,  1997,  1996,  6074,  1998,\n",
      "         2003,  2649,  2011,  1037,   100,   100,  1996,   100,  2492,  2003,\n",
      "         9950,  2011,  2019,  8522,  1997,  4367,  9529,  2005,  2943,  8381,\n",
      "         1998,  2005,  1996, 10178,  3446,  1999,  1996,   100,  1996,  3276,\n",
      "         2090,  1996, 10178,  3446,  1998,  1996,  6911,  1999,  1996,  8331,\n",
      "         2003,  3024,  2011,  1037,   100,  8522,  2008,  6115,  2005,   100,\n",
      "        11508,  6074,  1998,  8381,  1997,   100,  2122,   100,  2247,  2007,\n",
      "         5680,  1997,   100,  3073,  1037, 22961,   100,  6412, 11643,  3161,\n",
      "        11508,   100,   100,  2004,  2019,  2943,   100,   100,   100,  8331,\n",
      "          100,  1996,  4525,  7704, 11658, 11380,  8677,  1996,   100,  1997,\n",
      "         3161, 11508,   100,   100,   100,   100,  1999,  2236,   100,   100])\n",
      "Tags List: tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
      "        1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
      "        1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0.], dtype=torch.float64)\n",
      "Noun Phrases: ['cytoskeleton', 'theory', 'theory', 'continuum,', 'macroscopic', 'collection', 'agents,', 'bulk', 'medium,', 'dissipation', 'energy', 'polar', 'gel', 'unit', 'average', 'unit', 'volume', 'point', 'macroscopic', 'directionality', 'polarization', 'polarization', 'field', 'equation', 'motion', 'accounting', 'energy', 'consumption', 'strain', 'rate', 'fluid.', 'relationship', 'strain', 'rate', 'stress', 'fluid', 'equation', 'consumption', 'energy.', 'conservation', 'momentum,', 'continuum', 'description', 'energy', 'consuming,', 'fluid', 'however,', 'the theory', 'the theory', 'the continuum,', 'a collection', 'the gel', 'the average', 'each point', 'the macroscopic', 'the', 'a polarization', 'the polarization', 'an equation', 'the strain', 'the fluid.', 'the relationship', 'the strain', 'the stress', 'the fluid', 'these', 'a continuum', 'an energy', 'the', 'energy-fueled', 'cortical cytoskeleton', 'viscous', 'active agents,', 'viscous bulk', 'internal', 'uniaxial polar', 'local volume', 'constitutive equation', 'polar', 'hydrodynamic description', 'viscous', 'non-newtonian fluid', 'differential', 'viscous', 'a viscous bulk', 'which internal', 'a constitutive equation', 'dissipation of energy', 'average of unit', 'equation of motion', 'accounting for energy', 'consumption of energy.', 'conservation of momentum,', 'gels are, however,', 'gels are, however,']\n",
      "\n",
      "Sample 15:\n",
      "Tokenized Text: tensor([ 1999,  2344,  2000,  3231,  3251,   100,  7060,  2064, 16636,  2030,\n",
      "        11598,  1996,  6888,  2189,  1997,  1037,   100,  2019,   100,  3185,\n",
      "        12528, 13931,  2001,  3223,  5398,  1997, 15281, 12599,  2429,  2000,\n",
      "         1996,  7603, 21527,  1999,  1996,  6888,   100,  1996,  2206,  7809,\n",
      "         6407,  2020,  8920,  2004,  2825,  4216,  2005,  1996,   100,  1996,\n",
      "         6832,  3185,  7809,   100,   100,  1998,  2143,   100,   100,   100,\n",
      "         2122,  2020, 15105,  2044,  3319,  2004,   100,  1996,  6614,  1997,\n",
      "         2023,  2817,  2003,  2000, 11598,  1996,  6888,  1999,  1996,  2143,\n",
      "          100,  1998,  1999,  1996,  2553,  1997,  1996, 15281,  1999,  1996,\n",
      "          100,  2053,  5746,  2003,  3024,  2029,  8357,  1996, 15281,   100,\n",
      "         1999,  1996,  2553,  1997,  1996,  2143,   100,   100,  1996, 15281,\n",
      "         2024,  1999,  2413,  2738,  2084,   100,  1998,  2007,  2053,   100,\n",
      "         2029,  2073,  2036,  8357, 25622,  2144,  1996,  2913,  2024,  3344,\n",
      "         2041,  2007,  2394,  4092,   100,   100,  1996,  2143,   100,  4989,\n",
      "         2003,  2241,  2006,  1996,   100,  4180,  1997,  1996,  7984,  2004,\n",
      "         1999,  2087,  1997,  2068,  2045,  2003,  2053,  2189,  2029,  2003,\n",
      "         2036, 25622,  2004,   100,  2013,  2256,  3319,  1997,  2800,  7809,\n",
      "          100,  2009,  2001,  2179,  2008,  2012,  2556,  2045,  2003,  2053,\n",
      "         3115, 13931,  1997,   100,  3185, 15281,  2073,  1996,   100,   100,\n",
      "         3615,  2000,  1996,  3315,  3556,  1997,  1996,   100])\n",
      "Tags List: tensor([0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64)\n",
      "Noun Phrases: ['order', 'mood', 'music', 'movie,', 'movie', 'clip', 'corpus', 'consisting', 'emotion', 'mood', 'database', 'corpus:', 'movie', 'database', 'film', 'stim', 'however,', 'review', 'aim', 'study', 'mood', 'film', 'score,', 'case', 'emdb,', 'audio', 'case', 'film', 'stim', 'participants.', 'film', 'stim', 'selection', 'content', 'music', 'discussed.', 'review', 'database', 'collections,', 'present', 'corpus', 'movie', 'indexing', 'score', 'the mood', 'a movie,', 'the emotion', 'the mood', 'the corpus:', 'the aim', 'this study', 'the mood', 'the film', 'the case', 'the', 'the emdb,', 'no audio', 'the', 'the case', 'the film', 'the', 'no', 'the', 'the film', 'no music', 'the', 'haptic', 'affective movie', 'following database', 'possible', 'emotional movie', 'affective content', 'available database', 'standard corpus', 'affective movie', 'affective indexing', 'musical score', 'an affective movie', 'the following database', 'the emotional movie', 'the affective content', 'no standard corpus', 'the affective indexing', 'the musical score', 'consisting of']\n",
      "\n",
      "Sample 16:\n",
      "Tokenized Text: tensor([ 1996,  2193,  1997,  7885,  4146,  2001,  4359,  2011, 17739,  1996,\n",
      "         2176,  2087,  2590, 11709,  2005,   100,  2795,  1015,  2096,  1996,\n",
      "         3588, 11709,  2020,  2921,   100,  1996,   100,  4834,  3446,   100,\n",
      "         2001,  2562,  5377,  2012,   100,  2096,  1996,   100,  4834,  3446,\n",
      "          100,  2001,  9426,  2090,  1014,  1998,   100,  1996,  3778,  1999,\n",
      "         1996,   100,  4574,  2001,  4758,  2000,  2562,  1996,  3806,  4304,\n",
      "          100,  2144,  1996,  3778,  2038,  1037,  8793,  3466,  2006,   100,\n",
      "          100,  1996,  3778,   100,  2001,  9426,  2090,  2322,  1998,   100,\n",
      "         2009,  2323,  2022,  3264,  2008,  1996,  2291,  2001,  2448,  1999,\n",
      "         6882,  3778,  2491,   100,  2029, 10843,   100,  1996, 24420, 10764,\n",
      "         2000,  2562,  1037,  5377,  3778,  2076,   100,  1996, 17085,  2373,\n",
      "          100,  2001,  4964,  2012,   100,  2096,  1996, 13827,  2373,   100,\n",
      "         2001,  9426,  2090,  1014,  1998,   100,   100,  1996, 16305,  8057,\n",
      "         4860,   100,  2001,  4758,  2090,  2184,  1998,   100,  2023,  2640,\n",
      "         4504,  1999,  1037,  2440,   100, 11326,  1999,  2176,   100,  2073,\n",
      "         2093,  2415,  2685,  2020,  2109,  2000,  4638,  2005,   100,   100,\n",
      "         2073,  1996,   100,  2744,  1997,  1037, 16381,  2003,  2734,  2000,\n",
      "         9699,  1037,  9398,   100,  1996,  2561,  2193,  1997,  7885,  1999,\n",
      "         2023, 16437,  2003,   100,  2029,  2020, 13995,  2005,   100,   100,\n",
      "         1996,  7885,  1999,  1996,  2640,  2020,  3344,  2041,  1999,  6721,\n",
      "          100])\n",
      "Tags List: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0.], dtype=torch.float64)\n",
      "Noun Phrases: ['number', 'variation,', 'flow', 'rate', 'constant', 'flow', 'rate', 'pressure', 'etch', 'chamber', 'gas', 'density', 'stable.', 'pressure', 'effect', 'etch', 'characteristics,', 'pressure', '(p)', 'system', 'pressure', 'control', 'mode,', 'valve', 'pressure', 'coil', 'power', 'power', '(pb)', 'finally,', 'chuck', 'temperature', '(t)', 'design', 'screening', 'parameters,', 'center', 'curvature,', 'term', 'parameter', 'model.', 'number', 'setup', 'design', 'random', 'the number', 'the pressure', 'the etch', 'the gas', 'the pressure', 'the pressure', 'the system', 'the coil', 'this design', 'a parameter', 'this setup', 'the', 'the design', 'important', 'o2 flow', 'keep constant', 'sf6 flow', 'pronounced effect', 'automatic pressure', 'throttle valve', 'constant pressure', 'bias power', 'substrate chuck', 'factorial screening', 'quadratic curvature,', 'quadratic term', 'valid model.', 'total number', 'the o2 flow', 'the sf6 flow', 'a pronounced effect', 'the throttle valve', 'a constant pressure', 'the bias power', 'the substrate chuck', 'the quadratic term', 'a valid model.', 'the total number', 'number of', 'parameters for variation,', 'effect on etch', 'number of']\n",
      "\n",
      "Sample 17:\n",
      "Tokenized Text: tensor([ 3716,  2968,   100,  2003,  2028,  1997,  1996,   100,  2005,  2470,\n",
      "         1999,  1996,  2627,   100,  1999,  2087,   100,  1996,  2193,  1997,\n",
      "         5198,  1999,  1037,  3716,  2968,  2291,   100,  2003,  2200,   100,\n",
      "         1998,  2027,  2024,  2013,  9426,   100,  2130,  2060,   100,  1999,\n",
      "         2023,   100,  2070, 18419,  2043,  4493,  4725,  2055,  3229,  2491,\n",
      "         1998, 12832,  2024,  7333,  1999,   100,  2024, 16578,  2000,  2265,\n",
      "         2008,  2122,   100,  8107,  2342,  2000,  2022,   100,  2000,  9462,\n",
      "         1996,   100,  1997,  3025,   100,  2023,  3259, 17146,  2019,  3668,\n",
      "          100,  3229,  2491,   100,  4118,  1998,  1037,  8893, 12832,  3921,\n",
      "         2005,  3716,  2968,   100,   100,  1037,   100,  2291,  2003,  3591,\n",
      "         2000, 20410,  1996,  3818,   100])\n",
      "Tags List: tensor([1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
      "        0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1.],\n",
      "       dtype=torch.float64)\n",
      "Noun Phrases: ['knowledge', 'management', '(km)', 'research', 'decade.', 'number', 'knowledge', 'management', 'system', 'departments,', 'companies.', 'paper,', 'access', 'control', 'recommendation', 'kms', 'work,', 'paper', 'access', 'control', 'method', 'recommendation', 'approach', 'knowledge', 'management', 'system.', 'system', 'the', 'the number', 'a knowledge', 'this paper,', 'some', 'the', 'this paper', 'past decade.', 'varied departments,', 'other companies.', 'widely-used', 'previous work,', 'role-based access', 'hybrid recommendation', 'real-life system', 'the past decade.', 'these widely-used', 'a hybrid recommendation', 'a real-life system', 'hotspots for research', 'number of', 'methods about access', 'approach for knowledge']\n",
      "\n",
      "Sample 18:\n",
      "Tokenized Text: tensor([ 6555, 26383, 12375,  2001,  2034,  3818,  2011,   100,  3802,   100,\n",
      "          100,  2004,  2019,   100,  5331,  1997,  1996,  3115, 10763, 26383,\n",
      "        12375,  3921,  2004,  7528,  1999,  1996,   100,   100,  2009, 13585,\n",
      "         3674,  3760, 26383,  7341,  2000, 15796,  1037,  2172,  3469,  2028,\n",
      "         2029,  2089,  2025,  4906,  2046,  3638,  2478,  1996,  3151,   100,\n",
      "         2083,   100,  1996,  2832,   100,  1996,  5005, 16112,  1999, 10125,\n",
      "         9758,   100,  2011, 11566, 11165,  3463,  1998, 14985,   100,  2012,\n",
      "         1996,  2168,   100,  1996,  4304, 10197,   100,  5577,  1996,   100,\n",
      "         2030,  2181,  1997,   100,  1997,  1037, 11307,  2109,  2000,  3635,\n",
      "         1996,   100,  5140,  2013,  1996, 26383,  4949,  2076,  1037,  7205,\n",
      "        20065,   100,  2006,  1037,  3302,   100,   100,  1996,  4525,   100,\n",
      "         2003, 14267,  2000,  1996, 26383,   100,  1998,  1996, 20235,  2003,\n",
      "         4225,  2011,  1996,  3292,   100,  2000,  1996,   100, 26383,   100,\n",
      "         1999,  2023,   100,  2057,   100,  1996,  2744,  2000,  6235,  2593,\n",
      "         1996, 12177,  2030,  1996,  2193,  1997,  7205, 14754,  2005,  1037,\n",
      "         4304,   100,  5834,  2006,  1996,   100,   100,  2030,  2193,  1997,\n",
      "         7205,   100,  2003,  6360,  4359,  2000,   100,   100,  2004,   100,\n",
      "         2685,   100,  1996, 14830,  4304, 10035, 28314,  2000,  2019,   100,\n",
      "         5576,  1999,  1996,   100])\n",
      "Tags List: tensor([1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "        1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.], dtype=torch.float64)\n",
      "Noun Phrases: ['photon', 'mapping', 'hachisuka', 'et', 'al.', 'extension', 'photon', 'approach', 'radiance', 'photon', 'memory', 'approach.', 'iteration,', 'process', 'noise', 'inherent', 'monte', 'carlo', 'raytracing', 'them.', 'time,', 'density', 'estimate', 'bandwidth1bandwidth', 'support,', 'area', 'influence,', 'filter', 'photon', 'map', 'neighbour', 'lookup', 'surface', '(jensen,', 'irradiance', 'photon', 'density,', 'bandwidth', 'distance', '(radius)', 'photon', 'found.', 'paper,', 'term', 'radius', 'number', 'density', 'estimate,', '(radius', 'number', 'photons)', 'bias.', 'hachisuka', 'density', 'solution', 'the radiance', 'the process', 'the noise', 'the density', 'the support,', 'a filter', 'the', 'the photon', 'a surface', 'the photon', 'the bandwidth', 'the distance', 'this paper,', 'the term', 'the radius', 'the number', 'a density', 'the', 'progressive photon', 'iterative extension', 'static photon', 'traditional approach.', 'successive', 'same time,', 'implementation.1 (radius', 'nearest photons)', 'accumulated density', 'unbiased solution', 'an iterative extension', 'the traditional approach.', 'the same time,', 'the implementation.1 (radius', 'the accumulated density', 'an unbiased solution', 'approach. through iteration,', 'inherent in monte', 'area of influence,', 'bias. as hachisuka', 'photon mapping approach']\n",
      "\n",
      "Sample 19:\n",
      "Tokenized Text: tensor([ 1996, 20594,  1997,   100, 24535,  1999,   100,  1997,   100,   100,\n",
      "         2107,  2004,  2216,  5171,  1997,  2019,   100,  2064,  2022,  5147,\n",
      "        23364,  2478,  1037,  3674,  9539,   100,  2013,  1996,  2034,   100,\n",
      "         1997,   100,  4106,  2000,  2614, 20594,  1999,   100,  1997, 10806,\n",
      "         1998,  8206,  2892,  2930,  2302,  2812,   100,  2062,  3522,  8973,\n",
      "         2031,  3668,  1996,  4118,  2000,  3572,  2007,  6375,  2812,   100,\n",
      "         2812, 16499,   100,   100,  1997, 15275,  2892,   100,   100,  6375,\n",
      "         2812,   100,  1998,  6118,  9203,   100,  1996,   100,  3921,  2038,\n",
      "         1037,  2193,  1997,  5664, 12637,  2058,  2440, 15973,  4725,  2004,\n",
      "         2009,  2003, 28946, 10897,  2000,  5047,  3020, 13139,  1998,  1996,\n",
      "        15078, 11619,  2003,  2069,   100,  2062,  2084, 20177,  1996,   100,\n",
      "         2503,  1037,  3442,  5903,   100,  1996, 10640,  1998,   100,  1997,\n",
      "         1996,  3674,  9539,  3921,  2038,  2042,   100,  2114,   100,   100,\n",
      "         2005, 12689,   100, 22354,  1998,  6490,   100])\n",
      "Tags List: tensor([0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "        0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.], dtype=torch.float64)\n",
      "Noun Phrases: ['propagation', 'aeroengine,', 'application\\xa0[1]', 'analysis', 'propagation', 'rectangular', 'cross', 'section', 'flow,', 'method', 'mean', 'flow\\xa0[2],', 'flow\\xa0[3],', 'cross', 'section\\xa0[4]', 'mean', 'flow)', 'approach', 'number', 'complexity', 'parallel', 'accuracy', 'usefulness', 'approach', 'methods\\xa0[6]', 'aeroengine', 'the propagation', 'an aeroengine,', 'the method', 'the', 'a number', 'the', 'the accuracy', 'unsteady', 'slowly-varying', 'multiple', 'first application\\xa0[1]', 'circular cross', 'mean flow,', 'recent', 'uniform mean', 'arbitrary cross', 'uniform mean', 'distinct', 'numerical', 'computational complexity', 'straight parallel', 'multiple', 'finite-element methods\\xa0[6]', 'realistic aeroengine', 'acoustic', 'a multiple', 'the first application\\xa0[1]', 'the computational complexity', 'a straight parallel', 'the multiple', 'disturbances in', 'application\\xa0[1] of', 'propagation in']\n",
      "\n",
      "Sample 20:\n",
      "Tokenized Text: tensor([ 1037,  2430,  3160,  2013,  1996,  2391,  1997,  3193,  1997,  4517,\n",
      "         5584,  7336,  1996,  3431,  2000,  1996,   100,  1998,   100, 20611,\n",
      "         1997,  1037,  5391,   100,  2144,  2028,  2442,  4503,  1037, 10539,\n",
      "         2944,  1997,  2119,  1996,  2489, 20843,  1998,  1996,  8031,  1997,\n",
      "          100,  3225,  2013,  1996,   100,   100,  2023,  3291,  2003,  2738,\n",
      "          100,  2057, 13566,  2000,  3189,  2006,  2256,  4812,  1997,  2008,\n",
      "         3291,  1999,  2925,   100,  2005,  1996,   100,  2057,  2031,  4217,\n",
      "         2000, 19141,  1996,  5337,  4784,  2764,  2182,  2011, 11243,  2068,\n",
      "         2000,  1037,  9121,   100,  8419,  1996,   100, 20611,  1997,   100,\n",
      "        19490,   100,  3043,  1999,  2029,  2169,   100,  5683,  1037,   100,\n",
      "          100,   100,  1998,  1037,  9207,   100,   100,  2023,  2003,  1996,\n",
      "        18458,  1997,  1996,   100, 19780,   100,   100,  2029,  2038,  2042,\n",
      "         2109,  5147,  2000, 18422,  1996,  5144,  1997,  4517,  3043,  2004,\n",
      "         2092,  2004, 10713,   100,  2087,  3728,  2009,  2038,  2036,  2042,\n",
      "         2109,  2000, 18547,  2019,  4621,  4517,  2486,  2029,  2003,  2200,\n",
      "         2485,  2000,  1996,  4235,  2109,   100,  3523,   100,   100,  2008,\n",
      "         1999,   100,  1996,   100,  2024, 12302,  2011,  1996, 10210,   100,\n",
      "         2004,  2092,  2004,  3110,  1996,   100,   100,  1998,  9207,   100,\n",
      "         7013,  2011,  1996,  4193,   100,  1999,  1996,  2812,  2492,   100,\n",
      "         1996,   100,  8522,  2005,  1996,   100,  1999, 10709,   100,  3043,\n",
      "         2003,  2517,   100,   100])\n",
      "Tags List: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 1.], dtype=torch.float64)\n",
      "Noun Phrases: ['question', 'point', 'view', 'quark', 'bound', 'proton.', 'model', 'proton', 'binding', 'quark', 'level\\xa0[8],', 'problem', 'investigation', 'problem', 'work.', 'present,', 'toy', 'model,', 'quark', 'isospin', 'quark', 'matter', 'quark', 'potential,', '−vsq,', 'vector', 'potential,', 'vvq.', 'premise', 'quark–meson', 'model\\xa0[9]', 'matter', 'nuclei\\xa0[10,11].', 'force', 'skyrme', 'iii', 'force\\xa0[12].', 'qmc', 'mit', 'bag,', 'scalar', 'vector', 'nucleons.)', 'field', 'equation', 'quark', 'quark', 'matter', 'the point', 'the', 'the quark', 'a bound', 'the binding', 'the quark', 'this problem', 'that problem', 'the present,', 'a toy', 'the quark', 'each quark', 'a vector', 'the premise', 'the quark–meson', 'the', 'the', 'the mit', 'the quark', 'central question', 'nuclear', 'antiquark', 'reliable model', 'free proton', 'future work.', 'formal', 'symmetric quark', 'scalar potential,', 'nuclear matter', 'finite nuclei\\xa0[10,11].', 'nuclear force', 'mean-field scalar', 'mean field', 'dirac equation', 'infinite quark', 'as:', 'a central question', 'a reliable model', 'the free proton', 'the formal', 'a scalar potential,', 'the mean-field scalar', 'the mean field', 'the dirac equation', 'point of view', 'binding of', 'distributions of isospin', 'quark–meson coupling']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    tokenisedids, tagslist, noun_phrases = train_dataset[i]\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(\"Tokenized Text:\", tokenisedids)\n",
    "    print(\"Tags List:\", tagslist)\n",
    "    print(\"Noun Phrases:\", noun_phrases)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "\n",
    "# def extract_noun_phrases_spacy(text):\n",
    "#     nlp = spacy.load('en_core_web_sm')\n",
    "#     doc = nlp(text)\n",
    "#     noun_phrases = [chunk.text for chunk in doc.noun_chunks]\n",
    "#     return noun_phrases\n",
    "\n",
    "# # Example usage:\n",
    "# text = \"This is an example sentence with keywords like beautiful flowers and green grass.\"\n",
    "# noun_phrases_spacy = extract_noun_phrases_spacy(text)\n",
    "# print(noun_phrases_spacy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
