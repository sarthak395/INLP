{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "# from crfseg import CRF\n",
    "import torch.nn.functional as F\n",
    "import tqdm as tqdm\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Noun Phrases with REGEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_noun_phrases(pos_tags):\n",
    "    # Define patterns for noun phrases\n",
    "    patterns = [\n",
    "        r'NN.? VB.? NN.?',\n",
    "        r'NN.? IN NN.?',\n",
    "        r'DT JJ NN.?',\n",
    "        r'JJ NN.?',\n",
    "        r'DT NN.?',\n",
    "        r'NN.?'\n",
    "    ]\n",
    "\n",
    "    noun_phrases = []\n",
    "\n",
    "    # Convert the list of tuples to a space-separated string\n",
    "    pos_string = ' '.join(tag for word, tag in pos_tags)\n",
    "\n",
    "    # Check if the POS string matches any pattern\n",
    "    for pattern in patterns:\n",
    "        for match in re.finditer(pattern, pos_string):\n",
    "            # Get the start and end indices of the match\n",
    "            start, end = match.span()\n",
    "\n",
    "            # Convert the indices to word indices\n",
    "            start = pos_string[:start].count(' ')\n",
    "            end = pos_string[:end].count(' ')\n",
    "\n",
    "            # Extract the corresponding words\n",
    "            noun_phrase = ' '.join(word for word, tag in pos_tags[start:end])\n",
    "            \n",
    "            # Check if the noun phrase is not empty\n",
    "            if noun_phrase.strip():\n",
    "                noun_phrases.append(noun_phrase)\n",
    "\n",
    "    # Remove empty strings from the list\n",
    "    noun_phrases = [phrase for phrase in noun_phrases if phrase]\n",
    "\n",
    "    return noun_phrases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and creation of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "        # iterate through the files in the data directory\n",
    "        self.txtfiles = []\n",
    "        self.annfiles = []\n",
    "\n",
    "        for file in os.listdir(data_dir):\n",
    "            if file.endswith(\".txt\"):\n",
    "                self.txtfiles.append(file)\n",
    "        \n",
    "        self.tokeniser = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    def tokenise(self , text):\n",
    "        tokens = []  # List to store tokens\n",
    "        starting_offsets = []  # List to store starting offsets\n",
    "        current_token = ''  # Variable to store current token\n",
    "        offset = 0  # Starting offset\n",
    "\n",
    "        for char in text:\n",
    "            if char == ' ':\n",
    "                if current_token:  # If token is not empty\n",
    "                    tokens.append(current_token.lower())  # Append token in lowercase\n",
    "                    starting_offsets.append(offset - len(current_token))  # Store starting offset\n",
    "                    current_token = ''  # Reset current token\n",
    "                offset += 1  # Move offset to next character\n",
    "            else:\n",
    "                current_token += char  # Append character to current token\n",
    "                offset += 1  # Move offset to next character\n",
    "\n",
    "        # Handling the last token if it exists after the loop ends\n",
    "        if current_token:\n",
    "            tokens.append(current_token.lower())  # Append token in lowercase\n",
    "            starting_offsets.append(offset - len(current_token))  # Store starting offset\n",
    "\n",
    "        return starting_offsets , tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.txtfiles)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        txtfile = self.txtfiles[index]\n",
    "        sampleid = txtfile.split(\".\")[0]\n",
    "        \n",
    "        # read the text file\n",
    "        with open(os.path.join(self.data_dir, txtfile), 'r') as file:\n",
    "            txt = file.read()\n",
    "        \n",
    "        # read the annotation file\n",
    "        annfilename = sampleid + \".ann\"\n",
    "        with open(os.path.join(self.data_dir, annfilename), 'r') as file:\n",
    "            ann = file.read()\n",
    "        \n",
    "        offsets , tokenisedtxt = self.tokenise(txt)\n",
    "        tagslist = np.zeros(len(tokenisedtxt))\n",
    "        # now iterate through the ann file , in each line , divide into spaces and get the last word \n",
    "        # make tagslist[i] = 1 if the word is in the tokenisedtxt\n",
    "        for line in ann.split('\\n'):\n",
    "            if line == '':\n",
    "                continue\n",
    "            words = line.split()\n",
    "            if words[0][0] != 'T':\n",
    "                continue\n",
    "\n",
    "            ssofset = words[2]\n",
    "            endoffset = words[3]\n",
    "\n",
    "            # add a 1 to each index of tagslist for indexes where offset is between ssofset and endoffset (including both)\n",
    "            for i in range(len(offsets)):\n",
    "                if offsets[i] >= int(ssofset) and offsets[i] <= int(endoffset):\n",
    "                    tagslist[i] = 1\n",
    "        \n",
    "        # Convert tokens to IDs using BERT tokenizer\n",
    "        tokenisedids = self.tokeniser.convert_tokens_to_ids(tokenisedtxt)\n",
    "        \n",
    "        # Perform POS tagging\n",
    "        pos_tags = nltk.pos_tag(tokenisedtxt)\n",
    "        \n",
    "        # Extract noun phrases from POS tags\n",
    "        noun_phrases = extract_noun_phrases(pos_tags)\n",
    "        \n",
    "        return torch.tensor(tokenisedids), torch.tensor(tagslist), noun_phrases\n",
    "    \n",
    "    def collate_fn(self , batch):\n",
    "        # batch is a list of tuples\n",
    "        # each tuple has 3 tensors , one for tokenisedids, one for tagslist, and one for noun_phrases\n",
    "        # we need to return a tensor of tokenisedids, a tensor of tagslist, and a list of lists for noun_phrases\n",
    "        tokenisedids = []\n",
    "        tagslist = []\n",
    "        noun_phrases = []\n",
    "        for tup in batch:\n",
    "            tokenisedids.append(torch.tensor(tup[0]))\n",
    "            tagslist.append(torch.tensor(tup[1]))\n",
    "            noun_phrases.append(tup[2])\n",
    "        \n",
    "        tokenisedids = torch.nn.utils.rnn.pad_sequence(tokenisedids , batch_first=True , padding_value=0) \n",
    "        tagslist = torch.nn.utils.rnn.pad_sequence(tagslist , batch_first=True , padding_value=0)\n",
    "\n",
    "        tokenisedids = tokenisedids.type(torch.LongTensor)\n",
    "        tagslist = tagslist.type(torch.LongTensor)\n",
    "        \n",
    "        return tokenisedids , tagslist, noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset('/Users/ashnadua/Desktop/INLP-project/scienceie2017_train/train2')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True , collate_fn=train_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gold_keywords(ann_file):\n",
    "    gold_keywords = set()\n",
    "    with open(ann_file, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('T'):\n",
    "                parts = line.split()\n",
    "                keyword = ' '.join(parts[4:])\n",
    "                gold_keywords.add(keyword.lower())\n",
    "    return gold_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(extracted_keywords, gold_keywords):\n",
    "    correctly_extracted = len(extracted_keywords.intersection(gold_keywords))\n",
    "    total_gold_keywords = len(gold_keywords)\n",
    "    accuracy = correctly_extracted / total_gold_keywords if total_gold_keywords > 0 else 0\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ashnadua/Desktop/INLP-project/semeval_articles_test\n",
      "Average accuracy: 0.1841944963502457\n",
      "\n",
      "\n",
      "/Users/ashnadua/Desktop/INLP-project/scienceie2017_dev/dev\n",
      "Average accuracy: 0.18629873671989386\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dirs = ['/Users/ashnadua/Desktop/INLP-project/semeval_articles_test', '/Users/ashnadua/Desktop/INLP-project/scienceie2017_dev/dev']\n",
    "total_accuracy = 0\n",
    "total_files = 0\n",
    "\n",
    "for data_dir in data_dirs:\n",
    "    print(data_dir)\n",
    "    for file in os.listdir(data_dir):\n",
    "        if file.endswith(\".txt\"):\n",
    "            txt_path = os.path.join(data_dir, file)\n",
    "            ann_path = os.path.join(data_dir, file[:-3] + \"ann\")\n",
    "\n",
    "            with open(txt_path, 'r') as file:\n",
    "                text = file.read()\n",
    "\n",
    "            tokens = word_tokenize(text)\n",
    "            pos_tags = nltk.pos_tag(tokens)\n",
    "            noun_phrases = extract_noun_phrases(pos_tags)\n",
    "            noun_phrases = set(noun_phrases)  # Convert to set\n",
    "\n",
    "            gold_keywords = load_gold_keywords(ann_path)\n",
    "\n",
    "            accuracy = calculate_accuracy(noun_phrases, gold_keywords)\n",
    "            total_accuracy += accuracy\n",
    "            total_files += 1\n",
    "\n",
    "    # Calculate the average accuracy\n",
    "    average_accuracy = total_accuracy / total_files\n",
    "    print(f\"Average accuracy: {average_accuracy}\")\n",
    "\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_phrases = set(noun_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage for a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4\n"
     ]
    }
   ],
   "source": [
    "ann_file = '/Users/ashnadua/Desktop/INLP-project/scienceie2017_train/train2/S0010938X1500195X.ann' \n",
    "gold_keywords = load_gold_keywords(ann_file)\n",
    "\n",
    "accuracy = calculate_accuracy(noun_phrases, gold_keywords)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = list(noun_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poor oxidation\n",
      "the expected\n",
      "certain oxidation\n",
      "oxygen\n",
      "scale thickness\n",
      "typical temperature\n",
      "production\n",
      "the attempt\n",
      "role\n",
      "depth\n",
      "thickness\n",
      "expected\n",
      "the service\n",
      "barrier\n",
      "limited number\n",
      "this limitation\n",
      "number of\n",
      "careful study\n",
      "the demand\n",
      "alloys\n",
      "resistance\n",
      "ingress\n",
      "the production\n",
      "a certain oxidation\n",
      "production of\n",
      "depth of oxygen\n",
      "structural\n",
      "numerous\n",
      "range\n",
      "limitation\n",
      "pre-oxidation\n",
      "the role\n",
      "behavior\n",
      "limit\n",
      "composition\n",
      "compositional range\n",
      "a limited number\n",
      "use\n",
      "oxidation\n",
      "attempt\n",
      "literature\n",
      "development\n",
      "law\n",
      "development of\n",
      "]\n",
      "these\n",
      "temperature\n",
      "condition\n",
      "number\n",
      "the literature\n",
      "ti-based\n",
      "the major barrier\n",
      "service\n",
      "ti-based alloys\n",
      "major barrier\n",
      "the oxidation\n",
      "rate\n",
      "demand\n",
      "the typical temperature\n",
      "study\n",
      "that composition\n"
     ]
    }
   ],
   "source": [
    "for key in noun_phrases:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poor oxidation      thickness           resistance          pre-oxidation       literature          ti-based            that composition    \n",
      "the expected        expected            ingress             the role            development         the major barrier   \n",
      "certain oxidation   the service         the production      behavior            law                 service             \n",
      "oxygen              barrier             a certain oxidation limit               development of      ti-based alloys     \n",
      "scale thickness     limited number      production of       composition         ]                   major barrier       \n",
      "typical temperature this limitation     depth of oxygen     compositional range these               the oxidation       \n",
      "production          number of           structural          a limited number    temperature         rate                \n",
      "the attempt         careful study       numerous            use                 condition           demand              \n",
      "role                the demand          range               oxidation           number              the typical temperature\n",
      "depth               alloys              limitation          attempt             the literature      study               \n"
     ]
    }
   ],
   "source": [
    "keys_per_column = 10\n",
    "\n",
    "# Calculate the number of columns needed\n",
    "num_columns = (len(predicted) + keys_per_column - 1) // keys_per_column\n",
    "\n",
    "# Print keys in side-by-side columns\n",
    "for i in range(keys_per_column):\n",
    "    for j in range(num_columns):\n",
    "        idx = j * keys_per_column + i\n",
    "        if idx < len(predicted):\n",
    "            print(f\"{predicted[idx]:<20}\", end=\"\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
