{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = 'Corpus/Ulysses  James Joyce.txt'\n",
    "sentence = '''What is it now ?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentencetokeniser(text):\n",
    "    # split the text into sentences using regex a new line is encountered\n",
    "    sentences = re.split(r'\\n\\n', text)\n",
    "    # remove empty strings\n",
    "    sentences = list(filter(None, sentences))\n",
    "    # clean out the \\n\n",
    "    sentences = [sentence.replace('\\n', ' ') for sentence in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordtokeniser(sentence):\n",
    "    # split the sentences into words using regex a space is encountered\n",
    "    words = re.split(r' ', sentence)\n",
    "    # remove empty strings\n",
    "    words = list(filter(None, words))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacenumbers(words):\n",
    "    # replace numbers with a special token\n",
    "    for i in range(len(words)):\n",
    "        # starts and ends with a number with only numbers in between\n",
    "        if re.match(r'^\\d+$', words[i]):\n",
    "            words[i] = '<NUM>'\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectpunctuations(text):\n",
    "    punctuations = ['!', '?',]\n",
    "    for punctuation in punctuations:\n",
    "        text = text.replace(punctuation, ' ' + punctuation + '\\n\\n')\n",
    "    \n",
    "    seperators = [',' , ';', '(', ')', '[', ']', '{', '}', '\"', \"'\"]\n",
    "    for seperator in seperators:\n",
    "        text = text.replace(seperator, ' ' + seperator + ' ')\n",
    "    \n",
    "    # if before '.' , there is a lowercase letter and after tricky1 there is a space or the text ends , then it a fullstop . Only then replace it with a fullstop and a new line\n",
    "    text = re.sub(r'([a-z])\\. ', r'\\1 .\\n\\n', text)\n",
    "\n",
    "    # after the last word of text , if there is a fullstop , then replace it with a fullstop and a new line\n",
    "    text = re.sub(r'([a-z])\\.$', r'\\1 .\\n\\n', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacemailids(words):\n",
    "    # replace email ids with a special token\n",
    "    for i in range(len(words)):\n",
    "        if re.match(r'^\\w+@\\w+\\.\\w+$', words[i]):\n",
    "            words[i] = '<MAILID>'\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceURLs(words):\n",
    "    # replace URLs with a special token\n",
    "    starts = ['http://', 'https://', 'www.']\n",
    "    for i in range(len(words)):\n",
    "        for start in starts:\n",
    "            if words[i].startswith(start):\n",
    "                words[i] = '<URL>'\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacehash(words):\n",
    "    # replace hashtags with a special token\n",
    "    for i in range(len(words)):\n",
    "        # if word starts with # and contains atleast 1 letter\n",
    "        if words[i].startswith('#') and not re.match(r'^\\d+$', words[i][1:]):\n",
    "            words[i] = '<HASHTAG>'\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacementions(words):\n",
    "    # replace mentions with a special token\n",
    "    for i in range(len(words)):\n",
    "        # if word starts with @ and contains atleast 1 letter\n",
    "        if words[i].startswith('@') and not re.match(r'^\\d+$', words[i][1:]):\n",
    "            words[i] = '<MENTION>'\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceids(words):\n",
    "    # replace ids with a special token\n",
    "    for i in range(len(words)):\n",
    "        # if word starts with # and contains atleast 1 letter\n",
    "        if words[i].startswith('#') and re.match(r'^\\d+$', words[i][1:]):\n",
    "            words[i] = '<ID>'\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokeniser(text):\n",
    "    newtext = detectpunctuations(text)\n",
    "    sns = sentencetokeniser(newtext)\n",
    "    # FOR ALL SENTENCES , SPLIT INTO WORDS\n",
    "    allsns = []\n",
    "    for sn in sns:\n",
    "        words = wordtokeniser(sn)\n",
    "\n",
    "        # REPLACE WITH A SPECIAL TOKEN\n",
    "        words = replacenumbers(words)\n",
    "        words = replacemailids(words)\n",
    "        words = replaceURLs(words)\n",
    "        words = replacehash(words)\n",
    "        words = replacementions(words)\n",
    "        words = replaceids(words)\n",
    "\n",
    "        allsns.append(words)\n",
    "\n",
    "    return allsns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenise(tokenised , save_path):\n",
    "    num_sen = len(tokenised)\n",
    "    text = \"\"\n",
    "    for sen in tokenised:\n",
    "        line = ' '.join(sen)\n",
    "        text += line + \" \\n\"\n",
    "    \n",
    "    with open(save_path , \"w\") as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NgramModel(N , corpus_path):\n",
    "    text = open(corpus_path, 'r').read()\n",
    "    tokenised = tokeniser(text)\n",
    "\n",
    "    # N-gram model\n",
    "    tags = ['<BOS>' , '<EOS>']\n",
    "\n",
    "    # create a dictionary of all the N-grams\n",
    "    ngrams = {}\n",
    "    for sentence in tokenised:\n",
    "        # add N-1 <BOS> tags in the beginning\n",
    "        for i in range(N-1):\n",
    "            sentence.insert(0, tags[0])\n",
    "        # add 1 <EOS> tag in the end\n",
    "        sentence.append(tags[1])\n",
    "\n",
    "        for i in range(len(sentence)-N+1):\n",
    "            ngram = tuple(sentence[i:i+N])\n",
    "            if ngram not in ngrams:\n",
    "                ngrams[ngram] = 0\n",
    "            ngrams[ngram] += 1\n",
    "    \n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NgramProb(sentence , n , corpus_path):\n",
    "    ngram = NgramModel(n , corpus_path)\n",
    "    tokenised_text = tokeniser(sentence)\n",
    "    \n",
    "    num_sentences = len(tokenised_text)\n",
    "\n",
    "    for i in range(num_sentences):\n",
    "        # add N-1 <BOS> tags in the beginning\n",
    "        for j in range(n-1):\n",
    "            tokenised_text[i].insert(0, '<BOS>')\n",
    "        # add 1 <EOS> tag in the end\n",
    "        tokenised_text[i].append('<EOS>')\n",
    "\n",
    "    prob = 1\n",
    "    for sentence in tokenised_text:\n",
    "        for i in range(len(sentence)-n+1):\n",
    "            ngram_tuple = tuple(sentence[i:i+n])\n",
    "            if ngram_tuple in ngram:\n",
    "                prob *= ngram[ngram_tuple]\n",
    "            else:\n",
    "                prob *= 1e-6\n",
    "    \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EstimateParams(corpus_path):\n",
    "    params = [0 for i in range(3)]\n",
    "    trigram = NgramModel(3 , corpus_path)\n",
    "    bigram = NgramModel(2 , corpus_path)\n",
    "    unigram = NgramModel(1 , corpus_path)\n",
    "\n",
    "    keys = trigram.keys()\n",
    "    keys = list(keys)\n",
    "    corpus_size = len(keys)\n",
    "\n",
    "    # for all possible trigrams \n",
    "    for key in keys:\n",
    "        values = [0.0 for i in range(3)]\n",
    "        # if value doesn't exist , then it is 0\n",
    "\n",
    "        if (key[0] , key[1]) in bigram and bigram[(key[0] , key[1])] > 1:\n",
    "            values[2] = (trigram[key] - 1)/ (bigram[(key[0] , key[1])]-1)\n",
    "\n",
    "        if (key[1],) in unigram and unigram[(key[1],)] > 1:\n",
    "            values[1] = (bigram[(key[1] , key[2])] - 1)/ (unigram[(key[1],)]-1)\n",
    "        \n",
    "        if (key[2],) in unigram:\n",
    "            values[0] = (unigram[(key[2],)] - 1)/ (corpus_size-1)\n",
    "\n",
    "    # find the maximum value index\n",
    "        max_index = np.argmax(values)\n",
    "        params[max_index] += trigram[key]\n",
    "    \n",
    "    # normalise the params\n",
    "    params = np.array(params)\n",
    "    params = params/np.sum(params)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearInterpolation(corpus_path , sentence , N = 3 , multiple = False):\n",
    "    tags = ['<BOS>' , '<EOS>']\n",
    "    tokenised_text = tokeniser(sentence)\n",
    "    \n",
    "    # tokenised_text is a 2D list with each row representing a sentence and each column representing a word\n",
    "    num_sentences = len(tokenised_text)\n",
    "    for i in range(num_sentences):\n",
    "        # add N-1 <BOS> tags in the beginning\n",
    "        for j in range(N-1):\n",
    "            tokenised_text[i].insert(0, tags[0])\n",
    "        # add 1 <EOS> tag in the end\n",
    "        tokenised_text[i].append(tags[1])\n",
    "\n",
    "    params = EstimateParams(corpus_path)\n",
    "    trigram = NgramModel(3 , corpus_path)\n",
    "    bigram = NgramModel(2 , corpus_path)\n",
    "    unigram = NgramModel(1 , corpus_path)\n",
    "\n",
    "    keys = trigram.keys()\n",
    "    keys = list(keys)\n",
    "    # corpus size is sum of frequency of all unigrams\n",
    "    corpus_size = sum(unigram.values())\n",
    "\n",
    "    # break the sentence into trigrams\n",
    "    if multiple:\n",
    "        probs = []\n",
    "    else:\n",
    "        probs = 1\n",
    "    for j in range(num_sentences):\n",
    "        final_prob = 1\n",
    "        for i in range(len(tokenised_text[j])-N+1):\n",
    "            tri = tuple(tokenised_text[j][i:i+N])\n",
    "            bi = tuple(tokenised_text[j][i+1:i+N])\n",
    "            uni = tuple(tokenised_text[j][i+2:i+N])\n",
    "\n",
    "            prob_tri = params[2]*trigram[tri]/bigram[bi] if (bi in bigram and tri in trigram) else 0\n",
    "            prob_bi = params[1]*bigram[bi]/unigram[uni] if (uni in unigram and bi in bigram) else 0\n",
    "            prob_uni = params[0]*unigram[uni]/corpus_size if (uni in unigram) else 0\n",
    "            prob = prob_tri + prob_bi + prob_uni\n",
    "            final_prob *= prob\n",
    "        if multiple:\n",
    "            probs.append(final_prob)\n",
    "        else:\n",
    "            probs *= final_prob\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GoodTuringMethod(corpus_path, sentence , Ng = 3 , multiple = False):\n",
    "    tags = ['<BOS>' , '<EOS>']\n",
    "    tokenised_text = tokeniser(sentence)\n",
    "    \n",
    "    text = open(corpus_path, 'r').read()\n",
    "    tokenised_corpus = tokeniser(text)\n",
    "    words = []\n",
    "    for sen in tokenised_corpus:\n",
    "        words += sen\n",
    "    words = list(set(words))\n",
    "    \n",
    "    # tokenised_text is a 2D list with each row representing a sentence and each column representing a word\n",
    "    num_sentences = len(tokenised_text)\n",
    "    \n",
    "    for i in range(num_sentences):\n",
    "        # add N-1 <BOS> tags in the beginning\n",
    "        for j in range(Ng-1):\n",
    "            tokenised_text[i].insert(0, tags[0])\n",
    "        # add 1 <EOS> tag in the end\n",
    "        tokenised_text[i].append(tags[1])\n",
    "    \n",
    "    trigram = NgramModel(3 , corpus_path)\n",
    "    # max value of trigram.values()\n",
    "    max_freq = max(trigram.values())\n",
    "    freqoffreq = np.zeros((max_freq+1) , dtype=np.int32)\n",
    "\n",
    "    # find the frequency of each frequency\n",
    "    for key in trigram:\n",
    "        freqoffreq[trigram[key]] += 1\n",
    "    \n",
    "    # Normalising the freqoffreq\n",
    "    for r in range(2 , max_freq+1):\n",
    "        # find nearest nonzero indice on left\n",
    "        t = r-1\n",
    "        while t > 0 and freqoffreq[t] == 0:\n",
    "            t -= 1\n",
    "        q = r+1\n",
    "        while q < max_freq and freqoffreq[q] == 0:\n",
    "            q += 1\n",
    "        \n",
    "        freqoffreq[r] = freqoffreq[r] / (q-t)\n",
    "\n",
    "    \n",
    "    # handle Nr = 0\n",
    "    rs = np.arange(1 , max_freq+1) # r > 0\n",
    "    N = 0\n",
    "    for r in rs:\n",
    "        # print(freqoffreq[r])\n",
    "        if freqoffreq[r] == 0:\n",
    "            freqoffreq[r] = freqoffreq[r-1]\n",
    "        else:\n",
    "            N += r*freqoffreq[r]\n",
    "    \n",
    "    # Now , for Linear Good Turing Estimate (LGT) , estimate b\n",
    "    logNr = np.log(np.array(freqoffreq[1:]))\n",
    "    logr = np.log(rs)\n",
    "    model  = LinearRegression()\n",
    "    model.fit(logr.reshape(-1,1) , logNr)\n",
    "    b = model.coef_[0]\n",
    "    a = model.intercept_\n",
    "\n",
    "    # Now calculate prob of each r\n",
    "    # prob = np.zeros((max_freq+1))\n",
    "    # prob[0] = np.exp(a) / N\n",
    "    recounted_rs = np.zeros((max_freq+1))\n",
    "    recounted_rs[0] = np.exp(a) \n",
    "    for r in rs:\n",
    "        recounted_r = (r+1)*((1 + 1/r)**b)\n",
    "        recounted_rs[r] = recounted_r\n",
    "\n",
    "\n",
    "    # Now , we can calculate prob of sentence\n",
    "    if multiple:\n",
    "        probs = []\n",
    "    else:\n",
    "        probs = 1\n",
    "    for j in range(num_sentences):\n",
    "        final_prob = 1\n",
    "        for i in range(len(tokenised_text[j])-Ng+1):\n",
    "            tri = tuple(tokenised_text[j][i:i+Ng])\n",
    "            if tri in trigram:\n",
    "                r = trigram[tri]\n",
    "            else:\n",
    "                r = 0\n",
    "            num = recounted_rs[r]\n",
    "            # Now for all possible trigrams , calculate sum of recounted rs\n",
    "            den = 0\n",
    "            for word in words:\n",
    "                bi = tuple(tokenised_text[j][i+1:i+Ng] + [word])\n",
    "                if bi in trigram:\n",
    "                    den += recounted_rs[trigram[bi]]\n",
    "                else:\n",
    "                    den += recounted_rs[0]\n",
    "            prob = num/den\n",
    "            final_prob *= prob\n",
    "        if multiple:\n",
    "            probs.append(final_prob)\n",
    "        else:\n",
    "            probs *= final_prob\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(sentence , corpus_path , model = 'good-turing' , multiple = False):\n",
    "    tokenised = tokeniser(sentence)\n",
    "    num_words = len(tokenised[0])\n",
    "    print(\"Done tokenising\")\n",
    "    \n",
    "    if model == 'good-turing':\n",
    "        prob = GoodTuringMethod(corpus_path , sentence , multiple = multiple)\n",
    "    else:\n",
    "        prob = LinearInterpolation(corpus_path , sentence , multiple = multiple)\n",
    "    \n",
    "    if multiple:\n",
    "        per = []\n",
    "        # open a file\n",
    "        file = open('perplexity.txt', 'w')\n",
    "        print(\"Opened file\")\n",
    "    else:\n",
    "        per = np.inf\n",
    "    if multiple:\n",
    "        num_sentences = len(prob)\n",
    "        for i in range(num_sentences):\n",
    "            num_words = len(tokenised[i])\n",
    "            per.append(prob[i] ** (-1/num_words))\n",
    "            sen = ' '.join(tokenised[i])\n",
    "            file.write(sen + \" : \" + str(per[i]) + \"\\n\")\n",
    "    elif num_words > 0:\n",
    "        per = prob ** (-1/num_words)\n",
    "    return per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WordPrediction(sentence , corpus_path , ng , k , model = None):\n",
    "    tokenised = tokeniser(sentence)\n",
    "\n",
    "    # find all words in the corpus_path\n",
    "    text = open(corpus_path, 'r').read()\n",
    "    tokenised_corpus = tokeniser(text)\n",
    "    words = []\n",
    "    punctuations = ['!', '?','.' , ',' , ';', '(', ')', '[', ']', '{', '}', '\"', \"'\"]\n",
    "    for sentence in tokenised_corpus:\n",
    "        # remove punctuations\n",
    "        for punctuation in punctuations:\n",
    "            while punctuation in sentence:\n",
    "                sentence.remove(punctuation)\n",
    "        words += sentence\n",
    "    \n",
    "    # find the 'k' most probable words , use max-heap with size 'k'\n",
    "    probs = []\n",
    "    for word in words:\n",
    "        prob = 0\n",
    "        if model == 'good-turing':\n",
    "            prob = GoodTuringMethod(corpus_path , sentence + ' ' + word , ng)\n",
    "        elif model == 'linear-interpolation':\n",
    "            prob = LinearInterpolation(corpus_path , sentence + ' ' + word , ng)\n",
    "        else:\n",
    "            prob = NgramProb(sentence + ' ' + word , ng , corpus_path)\n",
    "        \n",
    "        probs.append(prob)\n",
    "    \n",
    "    probs = np.array(probs)\n",
    "    max_indices = np.argsort(probs)[-k:]\n",
    "    max_words = []\n",
    "    for index in max_indices:\n",
    "        max_words.append(words[index])\n",
    "    \n",
    "    return max_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.286909689231421e-05\n"
     ]
    }
   ],
   "source": [
    "prob = LinearInterpolation(corpus_path , sentence , N = 3)\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.406532478556684e-27\n"
     ]
    }
   ],
   "source": [
    "prob = GoodTuringMethod(corpus_path , sentence , Ng = 3)\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create Train and Test Corpus\n",
    "text = open(corpus_path, 'r').read()\n",
    "all_sentences = tokeniser(text)\n",
    "\n",
    "test_sentences = all_sentences[:1000]\n",
    "train_sentences = all_sentences[1000:]\n",
    "\n",
    "detokenise(test_sentences , 'Corpus/Ulysses  James Joyce_test.txt')\n",
    "detokenise(train_sentences , 'Corpus/Ulysses  James Joyce_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences :  23682\n",
      "Done tokenising\n",
      "Opened file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p6/7ftzfy7d35bcpx9rnxrn15cw0000gn/T/ipykernel_2229/1917266451.py:22: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  per.append(prob[i] ** (-1/num_words))\n"
     ]
    }
   ],
   "source": [
    "# Now computing perplexity of all sentences in the corpus\n",
    "print(\"Number of sentences : \" , len(all_sentences))\n",
    "per = compute_perplexity(text, 'Corpus/Pride and Prejudice - Jane Austen_train.txt' , model = 'good-turing' , multiple = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean perplexity :  33073710.038488723\n"
     ]
    }
   ],
   "source": [
    "# iterate through all sentences , and write perplexity in space , store in text file\n",
    "persum = 0 \n",
    "n = 0\n",
    "with open('per_good_ulysses_train.txt' , 'w') as f:\n",
    "    for i in range(len(train_sentences)):\n",
    "        index = i+1000\n",
    "        if per[index] == np.inf:\n",
    "            continue\n",
    "        else:\n",
    "            sen = ' '.join(all_sentences[index])\n",
    "            cont = sen + ' ' + str(per[index]) + '\\n'\n",
    "            n += 1\n",
    "            persum += per[index]\n",
    "            f.write(' '.join(cont))\n",
    "print(\"Mean perplexity : \" , persum/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
